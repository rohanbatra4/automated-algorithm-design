'''Name:''' Rohan Batra

'''Email:''' [mailto:rbatra34@gatech.edu]

'''Cell Phone:''' 470-909-8524

'''VIP:''' Automated Algorithm Design. Third semester in the VIP.

'''Sub-team:''' NLP <br><br>
'''Sub-Team Members:''' Rohan Batra, Aditya Kumaram, Manas Harbola, Runjie Chen, Ruchi Patel, Amish Saini, Aditi Prakash, Dhruv Patel, Tian Sun

'''Interests:''' Artificial Intelligence, Machine Learning, Soccer, Guitar
= Fall 2022 =
== Week 16: December 5th - December 9th(2022) == 
===Outline of class notes===
'''In-class notes:'''
* Will now pass on our primitives work to refinements team, so that they can get EMADE runs done.
* Ten minute discussion: Went over trying to finish of DeBERTa too as now RoBERTa is working (evaluated).
* Will start working on final distributions.
* No SCRUMs, just doing work now.
* Finished DeBERTa during class:
https://github.gatech.edu/sleone6/emade/commit/731b0b94ecba9229749ea3d3a63cbe536ffc610a
* Seeing if we should still add any other or if these may be enough for getting good EMADE evaluations.
* I finished adding slides for ALBERT and RoBERTa
'''Sub-team meeting notes:'''
* Went over a dry-run
* Slides time and content is looking good, just adding experiment results
'''Individual notes:'''
* Refined slides for ALBERT and RoBERTa.
* I will be going over these slides in the presentation. No other major work left this week. Presentation on Friday
* We got good results for DistilBERT. Got results for other models too which were decent but DistilBERT was giving better individuals.
'''Final Presentation Day:'''
* NLP:
** Our presentation went really well
** I went the first general huggingface embeddings expansion slide and the slides about ALBERT (all refinements results are now in the slide too as depicted below)
** Presentation: https://docs.google.com/presentation/d/1m5EhXWO_3TvJq_tSCfL8RyEWNs9WzZFXhkIUUrOXsms/edit?usp=sharing
** Presentation ended on time
** Got some feedback which was good and suggestions like do more EMADE setups for the team, work more on DEAP mutation functions to select required ones, work on PyTorch compatibility, and try to get over 8 hour job cap on PACE to get results faster(more runs and if it is going to fail it should happen quick). The team continuing next semester got some other general feedback to work on.
** Great work overall!
* Stocks:
** Literature review into portfolio optimization and price trend prediction
** Two parts:
*** Old team work
*** Working on new ideas (a little confusing slide with the diagram)
** New sources of data -- Technical indicators, ratios, accounting data
** For new data they settled on Volume, RSI, and OBV. I liked the explanation of how these indicators work.
** Data preprocessing -- Current problem is that data consisted entries at very different scales.
** Solution: Preprocess price and volume data so they are with scale.
** For price: Natural log of todays price/yesterday’s price.
** For volume: Scaled by average volume of past 50 days.
** Went over graphs and results. I liked how they dedicated a slide to this to explain better pareto front results from midterm. (Lower AUCs)
** Improved a lot from midterms.
** To do: Backtest individuals and look into graph anomalies
** Till now, I like the way the presentation is going. Want to see more experiments with EMADE for RL work which they are probably covering in the reinforcement learning subteam slides now. Overall, their results till now did seem good and showed good data segmentation!
** Went over a research paper.
** Used sharpe ratio and built evaluation functions.
** More data preprocessing for RL primitive (NPZ files)
** Used softmax activation function for "Deep Learning for Portfolio Optimization" Keras model
** Tried to decrease volatility.
** Got a few issues with creating this new model -- Sharpe ratio loss issues and LSTM layer problems.
** Integrated primitive with EMADE! Train model -> portfolio weight -> put back into EMADE data pair (issues of hard coding)
** Tried varying evaluation functions now. Mentioned examples of few ratios like Sortino.
** Went over Treynor Ratio exploration for eval function.
** Future work: Implement treynor ratio and complete EMADE run, evolve hyperparameter, and add/remove layers like NAS.
** Keeping transaction costs in mind! Good thought.
** Got more preprocessing feedback.
* Image processing:
** Recap of problem: Chest x-ray set with instances of 14 different diseases. Want to identify the diseases correctly though training models and EMADE. Same research paper as last semester.
** Evaluation method: Good explanation on what the curve depicts.
** Mentioned all issues of overfitting from last semester. Good way to introduce the work of this semester! Data leakage in previous datasets. Goal is to stay away from 0.5 in metrics
** Went over new primitives that they have introduced: conv2dstridesize can now pass either (1, 1) pr (2, 2) stride sizes onto MaxPool or Conv2d layers
** Wrote script to parse ADF to regenerate tree EMADE could parse. Created GradCam scripts to highlight image NNs.
** ADF script has few updates which are now working.
** Better use of CSVs for pareto frontier.
** Talked about problems of Lexicase selection. DEAP implementation is different from pure form of Lexicase in the original paper and weren't getting good results with this method.
** Good explanation of Lexicase. I feel it is important to go over such selection methods rather than just stating them as it help other teams understand.
** Current implementation progress for solving the issues: Needed access to the training images and need to be able to evaluate the individual trees inside the selection function. Since, they started with this a little later they only have a preliminary implementation ready and they believe it should work start of next semester.
** Preprocessing dataset: Preprocessed all images by sampling, resizing and making sure images don't appear in test and train.
** Difference in labelling methods was explained well.
** Liked the tabular explanation of data preprocessing and augmentation.
** Old code: Higher mutation rate: No added primitives, architecture or seeding. Faced problems and database did not seem to be able to find individuals. Got this issue due to versioning probably. Results weren't too good (got to 39 generations and a lot of hard coding)
** Tried more changes and came up with new code: Higher mutation rate: Doubled mutation rate and got some improvement but not too much.
** Evaluation method: F1 vs AUC
** No elite pool: replaced with parents to prevent evolution from getting stuck. Bad results.
** Till now, their explanations are pretty decent. Results for them don't seem to meet their expectations though.
** Mentioned ResNet and found a solution that allowed them to add more layers for better performance.
** Facing some issues with ResNet and mentioned that skipping layers in EMADE could help.
** Transformers in image processing.
** Had graphical representations for transfer learning.
** Evolutionary analysis of trees and pareto front graphs. Export CSVs for individuals and getting these Pareto front tables.
** All graphs increased increased and then plateau. Concerning results and their research paper is still showing better results currently.
** Compared their model with DenseNet-121. Can definitely improve their model significantly.
** Went over current issues such as new primitives needing better models need more memory, Pace kept giving OOM errors, etc.
** Went over future work which seems quite promising.
** Overall, I liked their explanations. I believe they are aware that their results and models can definitely be improved a lot which they can look into. Good amount of experimenting and valuable work!
* Scoliosis Measurements:
** Explained scoliosis first. Mentioned importance of Cobb angle. Good start for anyone who needs to understand what the team is about.
** Datasets: AASCE and Shriners. From mid term, they now have 1000 images from Shriner's 
** 300 have cobb angle evaluated.
** Data packing: Packed into npz file format
** They were earlier on setting up EMADE on Azure and were with Vertebrae landmark detection challenge winner. Have made changes since.
** Goal for final was to have modular example to show -- pipeline feeding set of images into cobb angle evaluation model after processing.
** They are on Azure provide by Shriner's and are hence using MSSQL (AzureSQL): had some driver issues
** Went over AzureSQL DB setup and mapping to this SQL from MySQL. Good explanation by going over database steps.
** Cropping schema: Start manually and then automate the process.
** Made a cropping primitive: added to GPFramework folder.
** Fixed dependency issues
** Unit tested the cropping primitive. Good way to find out whether they are implementing correctly.
** Got a better focussed and cropped image.
** Edge sharpening technique explained. Three image enhancement techniques.
** Depicted examples using images. Helped comprehend well.
** Primitives from edge sharpening use RegistryWrapperS.
** Model primitives: Vertebra-Focused Landmark Detection Model
** Used state-of-the-art models. Earlier work.
** They worked with VFL Model Primitive Progress and hoping to implement Seg4Reg model in EMADE in the future.
** Used PSPNet for image segmentation.
** Centerline extraction Cascaded NNs. Derives centerline from image. Postprocessing step ensure continuity of the curve, detecting sides of the centerline and finally selecting points.
** U-net model was also used. Made some good progress but still can work on developing bounding boxes code and/or vertebrae localization to specifically adapt to scoliosis problem.
** Till now, they are explaining very well and have talked about interesting models but would have liked to see more visual representation of results.
** Accurate Automated Keypoint Detection has 2 methods: RetinaNet & HR-NET is used by one and the other uses simple basline.
** Finally a fusion method merges above to methods.
** Their future work involves preprocessing (better image cropping and edge cropping)
** They have been going over multiple models, which took type to understand and implement. Their goal was to come up with pre-trained weights from all models, but they good only implement Vertebrae focussed landmark model yet.
** Mentioned other improvements, which they can focus on in the future.
** Overall, the work seems interesting and it would be nice to see in which direction the team finds the most success in the future.
** They do have interesting ideas involving different measurements (optimized from Cobb angle too)
* Overall, I had a great semester. Really enjoyed learning so much about NLP and working with the team members to make important code changes and explore intriguing models from huggingface. Will always be ready to help out in the future as all the research and experimenting has helped me learn a lot and I would love to always be a part of it!
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|December 5th, 2022
|December 9th, 2022
|December 9th, 2022
|-
|Complete final presentation
|Completed
|December 5th, 2022
|December 9th, 2022
|December 9th, 2022
|-
|Sub-team meeting
|Completed
|December 5th, 2022
|December 9th, 2022
|December 7th, 2022
|-
|Complete making slides and dry-run
|Completed
|December 5th, 2022
|December 9th, 2022
|December 7th, 2022
|}
== Week 15: November 28th - December 5th(2022) == 
===Outline of class notes===
'''In-class notes:'''
* Ten minute discussion
* Error faced with RoBERTa, ValueError: Input 0 of layer "model" is incompatible with the layer: expected shape=(None, 250), found shape=(None, 1, 250)
* Will try to flatten input now as soon as possible to get RoBERTa to evaluate.
* Started with SCRUMs
'''Sub-team #1 meeting notes:'''
* Dhruv and Amish are trying to solve issues of how to get embeddings errors fixed and then primitives ready to get models running through GloVe.
* Me, Tian, and Manas are working on fixing the RoBERTa issue as mentioned above.
* Main issue now is a memory error.
* Looking for a distilled version for RoBERTa called Tiny RoBERTa (https://huggingface.co/deepset/tinyroberta-squad2)
* Waiting on running ALBERT right now with a generalized tokenizer.
* ALBERT evaluated with new code!
'''Sub-team #2 meeting notes:'''
* Hackathon day
* I couldn't make it in person due to another final project for a class. Helped online
* RoBERTa worked finally. We had the correct idea of using the Tiny RoBERTa version.
* Dhruv and Amish have been working on other BERT models. Now that we our very sure how all BERT models work, they can implement them pretty quickly.
  ValueError: Output tensors of a Functional model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: 
  [[1.]. [1.]]

  Stack Trace: layerlist before pop   [['GloVe', <GPFramework.neural_network_methods.LayerList object at 0x2aabef38c1d0>, 
  <GPFramework.neural_network_methods.LayerList object at 0x2aabef38c210>], 'Fork', <keras.layers.core.dense.Dense object at 0x2aabd2d68750>, 
  <keras.layers.reshaping.flatten.Flatten object at 0x2aabe7ae1550>, 'output'] 
* This was the current error they were facing.
* They are now implementing DeBERTa, ELECTRA, and CamemBERT.
* This implementation will be similar to RoBERTa.
* Focus is on DeBERTa which they are trying to get to work.
'''Individual notes:'''
* Ran some experiments to see whether Tiny RoBERTa is giving the same results as RoBERTa.
* Not any major evidence to show, just that according to huggingface, when I ran it locally, the evaluation for the same text segment, I got the same results.
* Most of the work for this semester, we all sit together and code and give each other ideas.
* Each of us have at least implemented one primitive each. I initially implemented CANINE. The same way started work on ALBERT, which was taken over then by Tian to complete. Manas worked on RoBERTa and I just helped out with that if he got stuck as I had already worked on implementing models as primitives earlier.
* Commits for correct implementation of RoBERTa: https://github.gatech.edu/sleone6/emade/commit/e286f1a309951c7a0adfb06e796720e524ec32b5
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|November 28th, 2022
|December 5th, 2022
|December 4th, 2022
|-
|Implement RoBERTa
|Completed
|November 28th, 2022
|December 5th, 2022
|December 4th, 2022
|-
|Sub-team #1 meeting
|Completed
|November 28th, 2022
|December 5th, 2022
|December 2th, 2022
|-
|Sub-team #2 meeting
|Completed
|November 28th, 2022
|December 5th, 2022
|December 4th, 2022
|}
== Week 14: November 21st - November 28th(2022) == 
===Outline of class notes===
'''In-class notes:'''
* Ten minute discussion going over updates
* Refinement team ran EMADE.
* For PyTorch, models the inputs are PyTorch tensors, which was needed to be a keras tensors as outputs -- this is for embeddings -- still an open issue
* Refinement will discuss with Dr. Zutty about how to solve GPU issues
* We will work on converting PyTorch to Onnx and then try converting it to tensorflow.
* Other option could be removing splinter and using only tensorflow based models.
* Thanksgiving break this week
'''Individual Notes:'''
* Break week so whenever I got time to work I mainly worked on trying to convert to Onnx.
* Also, looked for new models that are tensorflow based in order to get some results in since the final presentation is coming up and we need to be sure we have some more results as although we have made a lot of significant code changes in this half of the semester, we don't have too many evaluated results apart from DistilBERT and ALBERT.
* Found new model with the team called RoBERTa.
* Onnx conversions gave me a type error, but will try working with Manas and Tian in the sub-team meeting after RoBERTa is implemented as getting results now is a priority: https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html and https://deci.ai/blog/how-to-convert-a-pytorch-model-to-onnx/
* Most other code work is usually always done in team meetings as we spend about two to three hours on implementing models.
'''Sub-team meeting Notes:'''
* We didn't have our regular meeting this time as everyone was on break
* However, Manas and I decided to meet up on Sunday to get a new BERT type model (RoBERTa as mentioned) running as PyTorch and non-BERT models were giving us to many issues. 
* We are implementing RoBERTa (https://huggingface.co/docs/transformers/model_doc/roberta) with our new cleaned up code. It should be easier to implement since we have generalized all of our code, so adding a new model that doesn't require too many types of different inputs (like PyTorch models) shouldn't be much of an issue.
* Making code changes now for RoBERTa in the files mentioned in branch earlier (Commit link in last week's notebook entry shows which files).
* RoBERTa mainly has the same architecture as BERT; however, it uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a different pretraining scheme.
* RoBERTa implementation plus code changes with how it works on splitting QA strings and runs model.
 from transformers import RobertaTokenizer, RobertaForQuestionAnswering
 import torch
 tokenizer = RobertaTokenizer.from_pretrained("deepset/roberta-base-squad2")
 model = RobertaForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2")
 question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
 inputs = tokenizer(question, text, return_tensors="pt")
 with torch.no_grad():
     outputs = model(**inputs)
 answer_start_index = outputs.start_logits.argmax()
 answer_end_index = outputs.end_logits.argmax()
 predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
 tokenizer.decode(predict_answer_tokens)
* Running it locally first then adding as primitive
* Worked locally after using the correct environments mentioned on huggingface.
* Now we start what I mentioned earlier - implementing as primitive on EMADE.
* Added RoBERTa code changes: https://github.gatech.edu/sleone6/emade/commit/c4c78f54f79a790e6a4efe602bb7ba0b12e1b72d
* Will run on EMADE with Tian.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|November 21st, 2022
|November 28th, 2022
|November 27th, 2022
|-
|Implement RoBERTa
|In Progress
|November 21st, 2022
|November 28th, 2022
|November 27th, 2022
|-
|Sub-team meeting
|Completed
|November 21st, 2022
|November 28th, 2022
|November 27th, 2022
|}
== Week 13: November 14th - November 21st(2022) == 
===Outline of class notes===
'''In-class notes:'''
* Ten minute discussion going over updates
* Going over SCRUMs
* Discussing with Dr. Zutty, in today's meeting, how we could make certain code changes to ensure that we can generalize adding non-BERT models too.
'''Sub-team meeting #1 notes:'''
* Tried debugging the code as we did decide to make major changes to the code base now.
* The code in the nnlearnerqa does seem to be a little unorganized when trying to run any new model as a primitive. 
* Turns out that even ALBERT started to depend a lot on DistilBERT to run and was in a way 'pseudo' calling it's modules.
* We made some code changes, which I will add the commit for in this notebook entry
* Dhruv and Amish are now working on NAQNET
* Tian, Manas and I will meet again on Sunday, to cleanup the code base and then also add splinter to get that finally working. We will first try to work in such a way such that DistilBERT works with the cleaned code too.
'''Indvidual Notes:'''
* Did some hit and trial runs to try to get splinter to run.
* Turns out the tokenization for SPLINTER is very slow, so looked for the a faster way to do it.
* Even the module it was accessing was giving issues to switched to: class transformers.SplinterTokenizerFast and class transformers.SplinterForQuestionAnswering for solving the two issues.
* Again, most of the work I do alone now is just going over some ways to solve the debugging issues we are facing, but most of the work is done with Manas and Tian on Sunday, as we sit for about 2-3 hours to work on these issues together.
* I came up with some ideas to make some code changes by using dictionaries directly for tokenization, which is now done and is shown in the git commits. All commits are done by Manas as only he has permission, but the code is written by Manas, Tian and I.
'''Sub-team meeting #2 notes:'''
* Working on getting DistilBERT to evaluate after we added some fixing cleanup code changes.
* Most of this was mainly to actually get EMADE to call the correct library with simple a dictionary data structure and use the model (when configured correctly with EMADE) to evaluate individuals after being connected to the dataset.
* DistilBERT is running right now and seems to be working without any errors, so hopefully it evaluates.
* Meanwhile Manas and I are adding some of the older commits to a new branch called nlp-splinter-primitive.
* We are also starting to figure out how we can then get splinter to start working with correct tokenization as mentioned in the above individual notes.
* Technically, we have already figured it out for splinter too as if this DistilBERT evaluation then splinter should be able to work in a similar fashion.
* So, the main difference in this new code is that earlier, the code used if else statements to add new models as layers and that pass in its attributes/arguments that the model required. Now, to improve the code design, we are using a dictionary to successfully get in the model with its arguments. The if else statements were too dependent on matching the names of the models and were complex when new models were needed. But for this new method, the dictionary allows us to easily add and access models and aren't as much dependent on just the names. We can easily now get the correct model with arguments in EMADE. This idea will be more clear when I add the new commits we have made.
* DistilBERT successfully evaluated! And this time we are sure it's calling the right model and evaluating it correctly with help of huggingface
* Now, trying for SPLINTER.
* Trying out some fixes from huggingface code since we got attribute errors.
* The output of the BERT tokenizer is different type from SPLINTER -- encode() function return type for SPLINTER is a list which we have to work with in a different way (its cleaner but BERT gave us a different type)
* We are thinking of preventing encoding then to access input_ids that we need with the dictionary format that it gives without encoding.
* Now, working on finding a better way to generalize taking in the correct input format for different models.
* Commits: https://github.gatech.edu/sleone6/emade/commits/nlp-splinter-primitive
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|November 14th, 2022
|November 21st, 2022
|November 20th, 2022
|-
|Cleanup code base
|Completed
|November 14th, 2022
|November 21st, 2022
|November 20th, 2022
|-
|Sub-team meeting #1
|Completed
|November 14th, 2022
|November 21st, 2022
|November 18th, 2022
|-
|Sub-team meeting #2
|Completed
|November 14th, 2022
|November 21st, 2022
|November 20th, 2022
|-
|Evaluate SPLINTER
|In progress
|November 14th, 2022
|November 21st, 2022
|November 20th, 2022
|}
== Week 12: November 7th - November 14th(2022) == 
===Outline of class notes===
'''In-class notes:'''
* Ten minute discussion to go over updates
* Dhruv and Amish will be working on a different model for embeddings stuff: https://huggingface.co/allenai/naqanet
* We are working on Splinter
* Tian worked on the issue and we just solved it right now. Splinter required two inputs for layering in EMADE, but we had added one, so it seems to be running now.
* Going over SCRUMs now.
* For this week we can try on making a generalization method for adding models as primitives that are similar. Something like adding a text file or just the model URL to ensure that it automatically calls API endpoints for adding the model as primitives.
'''Individual notes:'''
* Again, since Manas, Tian and I are working on adding more transformer models to EMADE as primitives, most of our work was done together as a team by meeting in-person.
* Other than that, I did research on how to get SPLINTER to work. We thought it started running but turns out our input parameters for the model were wrong.
* The reason why ALBERT model worked so easily was because it seems like the BERT tokenizer for the raw input was already there, which helped us out.
* Hopefully, we don't face any PyTorch compatibility issues this time like we did for CANINE.
* Also, I will start adding git commit links to my notebook once we start getting models to run as a lot of the code right now is just for testing a lot of the things we are doing and we all meet in person so most changes our anyways made together. However, I will still add commits of the week to my notebook.
* Commits: https://github.gatech.edu/sleone6/emade/tree/nlp-splinter-primitive
'''Sub-team meeting #1 notes:'''
* In the first meeting, we just mainly discussed what research had everyone done till yet and planning how we can start generalizing this process of adding models.
* We all could run SPLINTER locally, but EMADE is giving input issues
* Decided to meet on Sunday for a second meeting in-person to complete SPLINTER and start generalizing this process.
* Errors for now:
** https://drive.google.com/file/d/1AVWl2GasgpmvfIUNmNtxvFi4Y5YwvLKX/view?usp=sharing
'''Sub-team meeting #2 notes:'''
* Tokenization issues
* It seems like that for BERT the tokenization process has been hardcoded, which is why generalization will also be a bit difficult using NN_LearnerQA
* Thinking of using simple if statements for now to tokenize SPLINTER specifically
* Another method we could use is to make like a wrapper file that does all the pre-processing for such model to give us the right input
* For now, we are exploring the EMADE codebase even more to find better methods of tokenizing so generalization can be easier.
* BERT tokenizer doesn't seem like an important function that was really being called but this doesn't work with SPLINTER.
* After more reading, it seems like NNLearner_QA is defined mainly for BERT models and we can't use any other model as such.
* To change this, the file will have to be modified significantly, unless we can find a better way to uniformly do this for all non-BERT models.
* We need to discuss with Dr. Zutty if we should start making these major code changes to allow other general models to run too, and then can start with this stuff next week.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|November 7th, 2022
|November 14th, 2022
|November 13th, 2022
|-
|Go over generalization techniques
|Completed
|November 7th, 2022
|November 14th, 2022
|November 13th, 2022
|-
|Sub-team meeting #1
|Completed
|November 7th, 2022
|November 14th, 2022
|November 11th, 2022
|-
|Sub-team meeting #2
|Completed
|November 7th, 2022
|November 14th, 2022
|November 13th, 2022
|-
|Understand EMADE better to check if non-BERT primitives are implemented easily
|Completed
|November 7th, 2022
|November 14th, 2022
|November 13th, 2022
|-
|Implement model as Primitive
|Errors
|November 7th, 2022
|November 14th, 2022
|November 13th, 2022
|}
== Week 11: October 31st - November 7th(2022) == 
===Outline of class notes===
'''In-class notes:'''
* Bootcamp students are joining now.
* Prior to SCRUMs discussion in the start was to mainly go over, which models we want to start implementing.
* Three students have joined NLP.
* We are onboarding new students.
* Now going over SCRUMs
* Looking to implement transformer models as primitives that are less like BERT to make sure there is diversity for next semester students in case they wish to go forward with a different path.
'''Individual Notes:'''
* For this week most of the work that even I did was mainly with Manas, Tian, and Dhruv. We all met in person to work on researching on huggingface transformers and how to convert them to primitives.
* Most of us our individually trying out the same kind of work for now, which follows these steps:
** Find a non-BERT type model
** Make sure it makes sense for our requirements of Q/A
** Run the model locally to ensure all dependencies work correctly and the code is actually valid
** If this local run works, we can then decide on implementing this model into EMADE, which is a good sign
** Implementation as a primitive then will begin, but the work this week was mainly the first three steps as none of the models yet seemed to be too compatible with the dependencies we require.
* We are doing more research on few models that I have mentioned in Sub-team #2 meeting notes.
* UPDATE: Going forward with Splinter! https://huggingface.co/docs/transformers/model_doc/splinter
'''Sub-team meeting #1 notes:'''
* Discussed which huggingface transformers to implement
* Few options:
** https://huggingface.co/allenai/bidaf-elmo?context=My+name+is+Sarah+and+I+live+in+London&question=Where+do+I+live%3F
** https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
** https://huggingface.co/docs/transformers/model_doc/openai-gpt
** https://huggingface.co/docs/transformers/model_doc/nllb
** https://huggingface.co/docs/transformers/model_doc/realm
* Trying to move away from BERT type models for diversification, so will be going forward with maybe one of the first two for now.
* Scheduled another meeting for implementing a model for 11/6.
* Finding the models and understanding which one would make sense for us took some time and research, but these were the few ones which we shortlisted.
'''Sub-team meeting #2 notes:'''
* https://huggingface.co/allenai/bidaf-elmo?context=My+name+is+Sarah+and+I+live+in+London&question=Where+do+I+live%3F this is the model we are going forward with
* We are now trying to install the model from huggingface to see if we can run it locally and then adding it to EMADE primitive.
* Installing allenai and rust for the same.
* While installing these modules, we got a lot of dependency issues
    ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of 
    the following dependency conflicts.
    tensorboard 2.6.0 requires tensorboard-data-server<0.7.0,>=0.6.0, which is not installed.
    nbsphinx 0.8.8 requires docutils, which is not installed.
    sphinx 4.4.0 requires alabaster<0.8,>=0.7, which is not installed.
    sphinx 4.4.0 requires babel>=1.3, which is not installed.
    sphinx 4.4.0 requires docutils<0.18,>=0.14, which is not installed.
    sphinx 4.4.0 requires imagesize, which is not installed.
    sphinx 4.4.0 requires snowballstemmer>=1.1, which is not installed.
    sphinx 4.4.0 requires sphinxcontrib-applehelp, which is not installed.
    sphinx 4.4.0 requires sphinxcontrib-devhelp, which is not installed.
    sphinx 4.4.0 requires sphinxcontrib-htmlhelp>=2.0.0, which is not installed.
    sphinx 4.4.0 requires sphinxcontrib-jsmath, which is not installed.
    sphinx 4.4.0 requires sphinxcontrib-qthelp, which is not installed.
    sphinx 4.4.0 requires sphinxcontrib-serializinghtml>=1.1.5, which is not installed.
    tensorflow 2.6.0 requires flatbuffers~=1.12.0, but you have flatbuffers 20210226132247 which is incompatible.
    tensorflow 2.6.0 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.
    tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.22.2 which is incompatible.
    tensorflow 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 4.4.0 which is incompatible.
    gpframework 1.0 requires opencv-contrib-python==4.4.0.42, but you have opencv-contrib-python 4.4.0.46 which is incompatible.'
* It seems more of like a cyclic dependency issue, which would then give a lot of issues with EMADE too.
* Now, looking for another model that is different from BERT type models.
* https://huggingface.co/docs/transformers/model_doc/splinter: trying this model now.
* Model worked successfully on local!
* Facing some tree primitive issues, but will have good news that Splinter is at least working and we are going forward with it.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|October 31st, 2022
|November 7th, 2022
|November 6th, 2022
|-
|Research on models
|Completed
|October 31st, 2022
|November 7th, 2022
|November 6th, 2022
|-
|Sub-team meeting #1
|Completed
|October 31st, 2022
|November 7th, 2022
|November 4th, 2022
|-
|Sub-team meeting #2
|Completed
|October 31st, 2022
|November 7th, 2022
|November 6th, 2022
|-
|Ran 1 finalized model locally
|Completed
|October 31st, 2022
|November 7th, 2022
|November 6th, 2022
|-
|Implement model as Primitive
|Errors
|October 31st, 2022
|November 7th, 2022
|November 7th, 2022
|}
== Week 10: October 24th - October 31st (2022) == 
===Outline of class notes===
'''In-class notes:'''
* Mid-Term presentations day
* I went over slides 5, 33, 34, and 35 in https://docs.google.com/presentation/d/1tbFkRSsZIXl_v_U5wkHllEe0NoQlG0e6eQqdjE_T4VA/edit?usp=sharing
* Presentations of all teams summarized and observed below:
* NLP:
** I believe our presentation went really well and as planned.
** Hopefully new members are interested to join as many members on our team currently are in their last semester of the VIP and it would be great if NLP continues the work we are doing.
* Scoliosis Measurements:
** Segmentation utilizes deep CNN and the main measurement is Cobb Angle.
** They mentioned how the VerSe model used last sem has different 3D inputs and didn't really solve the problem that they were looking to solve for segmentation.
** They stated using a script for image type conversion, which I do remember writing last semester, so I hope it's the same script!
** Now on AASCE paper and using three cobb angles per image.
** Create an upper and lower bound and figured that SMAPE is greater than absolute percentage error.
** MSE is included in the evaluation functions
** They mentioned use of multiple other models out of which U-NET was used for building architecture of CNNs.
** I believe even they are looking to add primitives for better image processing.
* Image Processing
** They beat the ChestXNet paper last sem.
** Their goal right now is to train models with EMADE to classify images better.
** 0 for ROC is good 0.5 is bad.
** They couldn't really reproduce the dataset last sem which was an issue.
** Used GradCAM for image processing, heatmap for prediction visualization.
** Found not much difference between CPU and GPU
** Implemented NSGA-III and Lexicase selection methods
** They are facing file type problems with Stand Alone tree evaluator.
** Can use csv files for evaluation
** ADP tree parser usage: Dictionary construction
** Faced multiple bugs with above parser like nested ADFs and extra parenthesis issues
** They are implementing ResNet -- provides skip layers that give an alternative path for the gradient allowing more layers.
** Testing with new datasets and data augmentation.
** Transformers are getting good results but goal is to modify EMADE accordingly.
* Bootcamp 5:
** For data preprocessing they removed the Tickets and Cabin columns, added integer prefix, and did some data standardization by diving by mean.
** Used Decision Tree, Random Forest, NaiveBayes, Logistic.
** Multiobjective didn't work too well and so they Tournament selection, single objective.
** Used common primitives and ran 32/33 generations
** Tried multiple mutation and evaluation functions.
** Used FP and DN as objectives and later with FPR and FNR.
** Created primitives for EMADE and did comparison of AUC over 60 generations between MOGP HoF and EMADE
* Bootcamp 4:
** Correlation Matrix for data preprocessing.
** Individuals had infinity fitness at generation 0.
** For MOGP they modified age and sex and made features boolean
** Used XGBoost, SGD, RFC, and MLP
** Used AdaBoost for most pareto indivuals.
* Bootcamp 1:
** K-Fold Cross Validation used
** For data preprocessing dropped Name, Ticket, and Cabin columns. Set index as PassengerID column.
** KNeighborsClassifier() for ML algorithm
** Got 100 generations to run after running all night
** MGOP was better
** Faced MySQL issues.
** Implemented varOr()
** Had 150 pareto individuals
* Bootcamp 2.5:
** Correlation Matrix for data preprocessing 
** High accuracy in few generations and EMADE had lowest AUC
** Added models as primitives in addition to other custom primitives
** Used random constants and got better False Negative rate from start
* Stocks:
** Mentioned how previous work, team that wrote a research paper, found that Overall Profit and Monte Carlo Distribution worked better than current best performing models.
** Link to their research paper: https://dl.acm.org/doi/10.1145/3520304.3529038
** Read papers for Deep Learning Portfolio Optimization Problems
** Training on next day predictions by sector and trained models on each sector since stocks in different sectors have distinct behavior.
** Optimizes across the Sharpe ratio and reinforcement learning is tough to replicate in EMADE
** Faced few EMADE issues but goal is to add more data for better financial modelling.
** Sourced data from Yahoo
** AUC usually between 0.5 to 0.7, which indicates that there are no individuals that significantly beat random picking.
'''End of presentations'''
* Next task that i am thinking we can start off with:
** Explore other models that we can maybe try to add as layers (EMADE primitives) quickly to see how fast we can do so now and generalize the process
** Few models in mind: BART, RetriBERT
'''Sub-team meeting notes:'''
* Didn't really have a proper meeting as mid term just ended but we just went over future tasks on Slack and worked on the same over the weekend.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|October 24th, 2022
|October 31st, 2022
|October 24th, 2022
|-
|Figure out additional work
|Completed
|October 24th, 2022
|October 31st, 2022
|October 30th, 2022
|-
|Sub-team meeting
|Completed
|October 24th, 2022
|October 31st, 2022
|October 28th, 2022
|-
|Complete mid term presentation
|Completed
|October 24th, 2022
|October 31st, 2022
|October 24th, 2022
|}
== Week 9: October 17th - October 24th (2022) == 
===Outline of class notes===
'''In-class notes:'''
* No class this week due to Fall Break.
'''Sub-team meeting #1 notes:'''
* Mainly to go over the presentation and assign ever slides that they have to cover.
* Manas, Tian, and I got the slides for CANINE and ALBERT model for huggingface integration into EMADE ready.
'''Individual Notes:'''
* The slides that I have to go over and prepare for the presentation are mainly related to what are aim is by using Huggingface models and the reason why we first tried to get one model running.
* We wish to generalize the process.
* Tian and I will be covering these slides.
* Link to final presentation: https://docs.google.com/presentation/d/1tbFkRSsZIXl_v_U5wkHllEe0NoQlG0e6eQqdjE_T4VA/edit?usp=sharing
* Most work for this week was just refining the slides and preparing them for the mid-term presentation
'''Sub-team meeting #2 notes:'''
* This meeting was held on Sunday and the main aim was just to do a dry run of the presentation to ensure that we aren't going over time since our presentation is pretty long in terms of number of slides.
* I believe my part went pretty well and over all we don't really need to shorten our time too much but just a little bit.
* I will be covering four slides.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|October 17th, 2022
|October 24th, 2022
|October 23rd, 2022
|-
|Prepare for Mid-Term presentation
|Completed
|October 17th, 2022
|October 24th, 2022
|October 23rd, 2022
|-
|Sub-team meeting #1
|Completed
|October 17th, 2022
|October 24th, 2022
|October 21st, 2022
|-
|Sub-team meeting #2
|Completed
|October 17th, 2022
|October 24th, 2022
|October 23rd, 2022
|-
|Work on adding ALBERT model with Manas and Tian
|Completed
|October 17th, 2022
|October 24th, 2022
|October 20th, 2022
|}
== Week 8: October 10th - October 17th (2022) == 
===Outline of class notes===
'''In-class notes:'''
* Went over CANINE errors
* Will talk to Dr. Zutty after SCRUMs
* Going over SCRUMs
'''Sub-team meeting notes:'''
* Decided that CANINE may give us issues and hence we are going forward with using the ALBERT model. 
* We will definitely try to get CANINE to work too, but since mid term presentations are coming up we want some solid results.
* Since, we already know how to set up a huggingface transformer model know, setting up ALBERT shouldn't take time.
* This will be our task for the week
* I was travelling during this time so was mainly helping out on Slack for updating my work.
'''Individual Notes:'''
* This week and next week will be quite busy for me due to interviews, but I will try my best to help out with whatever possible code changes I can make to get ALBERT running on EMADE and run successfully using the stand-alone evaluator.
* All changes that Tian and I started making were very similar to everything we did for CANINE.
* We first got a definition for the model set up.
* Then moved on to adding ALBERT as a layer. 
* Few images are attached for reference.
* Images:
[[files/ALBERT1.png | height = 300px]][[files/ALBERT2.png | height = 70px]][[files/albert3.png | height = 70px]][[files/ALBERT4.png | height = 70px]]
* Few details about the ALBERT model and why we chose it:
** Extension to traditional BERT model with performance increase
** Memory usage reduction with parameter reduction techniques
*** Splitting the embedding matrix into two smaller matrices.
*** Cross-layer parameter sharing
*** Prevent parameter growing
** Increased performance with shorter training time
* We were finally able to run it using the stand alone evaluator and got some results that turned out to be slightly better than the last year's seeded individual.
* Results are as follows:
[[files/albert5.png | height = 250px]]
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|October 10th, 2022
|October 17th, 2022
|October 16th, 2022
|-
|Ran Stand Alone Tree Evaluator
|Completed
|October 10th, 2022
|October 17th, 2022
|October 14th, 2022
|-
|Sub-team meeting
|Completed
|October 10th, 2022
|October 17th, 2022
|October 14th, 2022
|-
|Complete adding CANINE as layer (working model)
|In progress
|October 10th, 2022
|October 17th, 2022
|October 17th, 2022
|-
|Work on adding ALBERT model with Manas and Tian
|Completed
|October 10th, 2022
|October 17th, 2022
|October 14th, 2022
|}
== Week 7: October 3rd - October 10th (2022) == 
===Outline of class notes===
'''In-class notes:'''
* Updated team with CANINE code changes
* Had some trouble in dataset usage
* Went over SCRUMs
* Talked to Professor Dr. Zutty about dataset usage with Tian.
* Using input_squad.xml for dataset (preprocessed with data.py)
'''Sub-team meeting notes:'''
* Decided to push all code changes that I made (for Canine) and dataset additions Tian made on Steven's fork. https://github.gatech.edu/sleone6/emade/commit/aea8c2205b873059fd46c56db62070c58f131f63
* Manas worked with us and he pushed all changes as Tian and I are new and don't have permissions yet.
* I added some more code changes https://github.gatech.edu/sleone6/emade/commit/2942483467387f5c30697e60e3f7b23168e13af3
** These code changes basically were to add CANINE to EMADE layers
** Added CANINE as primitive
** Added definition of CANINE transformer
** pset primitive layer added too
* We are trying to run StandAlone Tree Evaluator now to make sure CANINE works.
* Right now, I have been facing some issues related to M1 mac for running it
* We have finally figured out correct dataset to run tests on (squad) and will now work on running StandAlone
'''Individual Notes:'''
* Working with Aditya and Manas to fix StandAlone errors.
* Used this to fix M1 issues, https://developer.apple.com/metal/tensorflow-plugin/, which did give a lot of dependency issues but now work.
* Reinstalled conda environment to fix above dependency errors.
* Ran python3 src/GPFramework/standalone_tree_evaluator.py templates/input_squad.xml to run StandAlone Tree Evaluator.
* After fixing all dependency issues, we were able to run Stand Alone. No code errors yet and CANINE started tokenizing! But we are facing mysql errors now and database has to be configured. So Aditya and I will work on that and make a test table database and run it on local. Hopefully CANINE evaluates the data correctly.
* This will be our major task for the next week now as we want results before the mid-term presentation
* Error: sqlalchemy.exc.OperationalError: (_mysql_exceptions.OperationalError) (2006, "Unknown MySQL server host 'atl1-1-02-012-5-r' (0)")
(Background on this error at: https://sqlalche.me/e/14/e3q8)
    label array: [array([ 67.,  55., 128., ...,  97.,  36.,   1.]), array([ 70.,  57., 128., ...,  97.,  36.,   3.])]
    (1, 0)
    label array: [array([ 67.,  55., 128., ...,  97.,  36.,   1.]), array([ 70.,  57., 128., ...,  97.,  36.,   3.])]
    (1, 0)
    label array: [array([ 41.,  28.,  63., ...,  68., 171., 158.]), array([ 41.,  31.,  67., ...,  68., 172., 160.])]
    (1, 0)
    label array: [array([ 41.,  28.,  63., ...,  68., 171., 158.]), array([ 41.,  31.,  67., ...,  68., 172., 160.])]
    (1, 0)
As seen above CANINE kind of started working but faced database error.
* Work with Tian for MySql bug fixing:
    Remove everything in scratch/db
    Start a pace node using qsub command
    qsub-I -q pace-ice -l nodes=1:ppm=1,walltime=01:00:00
    Install and run MySQL instance on that node
    Open another terminal, ssh into the node(ssh atl1-1-02-012-9-r)
    Once you are in node, run MySQL -u root
    Then you should enter MySQL instance with root permission, run the following command to establish an user with remote connection:
    DELETE FROM mysql.user WHERE user='';
    GRANT ALL PRIVILEGES ON *.* TO 'USERNAME'@'%' WITH GRANT OPTION;
    FLUSH PRIVILEGES;
    now type exit to leave MySQL instance and test that your new user is established with mysql -u USERNAME
    After that you should configure the input template with appropriate mysql host, port user and database name(which you need to create)
    And now exit the node and run the standalone tree evaluator on another job instance. 
* Tian was able to connect to mysql database after this now we are going to test if CANINE works.
* Syntax error:
    NNLearnerQA(ARG0, ARG1, OutputLayer(FlattenLayer(DenseLayer(1, defaultActivation, 8, Canine(DistilBERTEmbeddingLayer(InputLayer(), 
    InputLayer()))))), 41, AdamOptimizer)
    NNLearnerQA(ARG0, ARG1, OutputLayer(FlattenLayer(DenseLayer(1, defaultActivation, 8, Canine(DistilBERTEmbeddingLayer(InputLayer(), 
    InputLayer()))))), 41, AdamOptimizer)
    Evaluating Individual NNLearnerQA(ARG0, ARG1, OutputLayer(FlattenLayer(DenseLayer(1, defaultActivation, 8, 
    Canine(DistilBERTEmbeddingLayer(InputLayer(), InputLayer()))))), 41, AdamOptimizer)
    Starting Evaluation of Individual: NNLearnerQA(ARG0, ARG1, OutputLayer(FlattenLayer(DenseLayer(1, defaultActivation, 8, 
    Canine(DistilBERTEmbeddingLayer(InputLayer(), InputLayer()))))), 41, AdamOptimizer)
    Hash of the Individual: 9d6f780f51afc99cb5311ba0649fd7249a19fc0db0901e219d4d8ec597aabe3d
    TimeStamp | 2022-10-08 17:41:41.207946
    Traceback (most recent call last):
    File "src/GPFramework/standalone_tree_evaluator.py", line 204, in <module>
       print(evaluation_wrapper(my_individual, dataset=0, database=database))
    File "src/GPFramework/standalone_tree_evaluator.py", line 45, in evaluation_wrapper
       individual, elapsed_time, error_string, retry = emade.evaluate_individual(individual, dataset)
     File "/storage/home/hpaceice1/tsun90/.conda/envs/emade/lib/python3.7/site-packages/GPFramework/EMADE.py", line 1694, in evaluate_individual
    func = _inst.toolbox.compile(expr=individual)
    File "/storage/home/hpaceice1/tsun90/.conda/envs/emade/lib/python3.7/site-packages/deap/gp.py", line 509, in compileADF
    for pset, subexpr in reversed(zip(psets, expr)):
    TypeError: 'zip' object is not reversible
* This should be fixed by changing it to reveresed(list(zip...))) as zip needs to be iterable.
* Updated deap to 1.3 to fix this issue. Working on switching to TFCANINE as EMADE primarily uses tensorflow instead of pytorch
* Tasks:
** Debugging
*** Errors:
    raise Exception(return_dict['error'] + ' ' + get_stack_from_queue(my_queue))
    Exception: Parameter config in `CanineModel(config)` should be an instance of class `PretrainedConfig`. To create a model from a pretrained model use `model = CanineModel.from_pretrained(PRETRAINED_MODEL_NAME)`Traceback (most recent call last):
    File "/storage/home/hpaceice1/tsun90/.conda/envs/emade/lib/python3.7/site-packages/GPFramework/EMADE.py", line 1598, in handleWorker
     result = future.result()
    File "/storage/home/hpaceice1/tsun90/.conda/envs/emade/lib/python3.7/concurrent/futures/_base.py", line 428, in result
     return self.__get_result()
    File "/storage/home/hpaceice1/tsun90/.conda/envs/emade/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
     raise self._exception
    File "/storage/home/hpaceice1/tsun90/.conda/envs/emade/lib/python3.7/concurrent/futures/thread.py", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
    File "<string>", line 1, in <lambda>
    File "/storage/home/hpaceice1/tsun90/.conda/envs/emade/lib/python3.7/site-packages/transformers/models/canine/modeling_canine.py", line 989, 
  in __init__
     super().__init__(config)
    File "/storage/home/hpaceice1/tsun90/.conda/envs/emade/lib/python3.7/site-packages/transformers/modeling_utils.py", line 943, in __init__
     f"Parameter config in `{self.__class__.__name__}(config)` should be an instance of class "
    ValueError: Parameter config in `CanineModel(config)` should be an instance of class `PretrainedConfig`. To create a model from a 
    pretrained model use `model = CanineModel.from_pretrained(PRETRAINED_MODEL_NAME)`
** Try to check if pytorch version can be fixed (talk to Professor Dr. Zutty)
** Work on a different tensorflow based model to get results as layering is almost ready to go
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|October 3rd, 2022
|October 10th, 2022
|October 8th, 2022
|-
|Ran Stand Alone Tree Evaluator
|Completed
|October 3rd, 2022
|October 10th, 2022
|October 8th, 2022
|-
|Sub-team meeting
|Completed
|October 3rd, 2022
|October 10th, 2022
|October 7th, 2022
|-
|Complete adding CANINE as layer (code changes)
|Completed
|October 3rd, 2022
|October 10th, 2022
|October 8th, 2022
|-
|Work on debugging with Manas and Aditya
|Completed
|October 3rd, 2022
|October 10th, 2022
|October 8th, 2022
|}
== Week 6: September 26th - October 3rd(2022) == 
===Outline of class notes===
'''In-class notes:'''
* Updated team about the CANINE definition error.
* Will require some help from some of the older members as although I can do it by myself as a new member too; however, if I work with one of the older members, we could wrap this up faster. But nevertheless it should be good to go soon.
* Went over SCRUMs
* Peer-evals start next week.
'''Sub-team meeting notes:'''
* Meeting was mainly focussed on embeddings work.
* Told Manas about my updates and am working on testing with the Stand Alone tree evaluator
* Debugging for my code is in progress.
* Tian and I usually work separately as the sub team meeting is mainly required for embeddings and refinement and the two of us focus EMADE primitives for huggingface transformers.
'''Individual Notes:'''
* Made the following additions in neural_network_methods.py
   * from transformers import TF_DPR_QUESTION_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST, CanineTokenizer, TFDistilBertModel,DistilBertTokenizer #, DistilBertConfig # requires TF >=2.2
   * def Canine(layerlist):
     layerlist.mylist.append("Canine")
     return layerlist
   * elif layer == 'Canine':
     #add Canine layer byb importing the canine modeling.py file and adding the layer
     curr_layer_list.append(CanineTokenizer.from_pretrained("google/canine-s"))
* Made the following additions in gp_framework_helper.py
   * pset.addPrimitive(nnm.Canine, [nnm.LayerList, nnm.LayerList], nnm.LayerList, name="Canine")
* Added new file for reference and use for QA
   from transformers import CanineTokenizer, CanineForQuestionAnswering
   import torch

   tokenizer = CanineTokenizer.from_pretrained("google/canine-s")
   model = CanineForQuestionAnswering.from_pretrained("google/canine-s")

   question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

   inputs = tokenizer(question, text, return_tensors="pt")
   with torch.no_grad():
   outputs = model(**inputs)

   answer_start_index = outputs.start_logits.argmax()
   answer_end_index = outputs.end_logits.argmax()

   predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
   tokenizer.decode(predict_answer_tokens)
* Made change to standalone tree evaluator but am getting illegal hardware instruction error
   strings_to_eval = ["NNLearnerQA(ARG0, ARG1, OutputLayer(FlattenLayer(DenseLayer(1, defaultActivation, 8, 
   Canine(DistilBERTEmbeddingLayer(InputLayer(), InputLayer()))))), 41, AdamOptimizer)"]
* Going to use https://developer.apple.com/metal/tensorflow-plugin/ to see if I can fix some m1 mac issues in case.
* Currently trying to run the datasets with Tian. Our code changes are done but we are trying to test it using the StandAlone Tree evaluator.
* Current errors:
    point_start_index = float(label[1:].split(',')[0])
    ValueError: could not convert string to float:
* Testing to be done with data on PACE:
    #PBS -N emade-standalone
    #PBS -l nodes=1:ppn=8
    #PBS -l pmem=4gb
    #PBS -l walltime=1:00:00
    #PBS -q pace-ice
    #PBS -j oe
    #PBS -o emade-standalone.out
    #PBS -M USERNAME@gatech.edu

    cd ~/emade
    echo "Started on `/bin/hostname`" # prints the name of the node job started on
    module load jdk/1.8.0_202
    module load openmpi/3.1.6
    export CC=gcc
    echo "before anaconda3"
    module load anaconda3/2021.05
    module load cuda/11.2
    module load tensorflow-gpu/2.6.0
    echo "before env activate"
    conda activate emade35
    python src/GPFramework/standalone_tree_evaluator.py templates/input_chestxray_multiclass.xml -tr /storage/home/hpaceice1/shared- 
    classes/materials/vip/AAD/ip_group/datasets/chest_xrays/chest_train.npz -te /storage/home/hpaceice1/shared- 
    classes/materials/vip/AAD/ip_group/datasets/chest_xrays/chest_test.npz # use your XML file 
* Basically, right now the squad dataset is unable to be parsed due to some issues, which we are trying to debug.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 26th, 2022
|October 3rd, 2022
|October 2nd, 2022
|-
|Research on CANINE
|Completed
|September 26th, 2022
|October 3rd, 2022
|September 30th, 2022
|-
|Sub-team meeting
|Completed
|September 26th, 2022
|October 3rd, 2022
|October 1st, 2022
|-
|Convert to primitive
|Completed
|September 26th, 2022
|October 3rd, 2022
|October 2nd, 2022
|-
|Test with StandAlone
|In progress
|September 26th, 2022
|October 3rd, 2022
|October 3rd, 2022
|}
== Week 5: September 19th - September 26th(2022) == 
===Outline of class notes===
'''In-class notes:'''
* Discussed more about huggingface model to EMADE primitives wrapping
* Went over SCRUMs
'''Individual Notes:'''
* First Research Paper: CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation
** Most of the NLP systems that are pipelined use a an explicit tokenization method. However, in this paper we read about a model that uses an efficient Tokenization free method.
** So, what are Tokenizers? Ans. They are carefully constructed expensive systems of language-specific rules. Or, are data driven algorithms such as Byte Pair Encoding or Sentence Piece.
** Removing this tokenization method could lead to certain problems due to replacing the subword vocabulary in a model like BERT  with a vocabulary made solely of individual characters—doing. Problems are mainly such of computational complexity and switching vocabulary leads to inefficiency. 
** CANINE is a large language encoder with a deep transformer. It efficiently works around character level and vocabulary based losses.
** [[files/CANINEModel.png | height = 250px]]
** The importance in the use of this model is built around the difference in preprocessing methods (represented by a sequence of integers).
** For this model, constructing downsampling function is important and has a different mathematical approach as mentioned below
** [[files/FirstPaperMath1.png | height = 50px]][[files/FirstPaperMath2.png | height = 50px]]
** While this could be enough, upsampling is also required in a different method and we reconstruct a character-wise output representation.
** Few parts after this, I did find slightly complex; however, I kind of started to understand the model better when compared to mBERT. A detail of the comparison is included in the image below, which really helped me understand the logic better.
** The table and text mentioned below depict the way in which CANINE improved over mBERT and show how it provides more opportunities for embeddings to comprehend text better.
**[[files/mBERTComparison.png | height = 250px]][[files/KfirstPaper.png | height = 250px]]
** Overall, CANINE helps remove many engineering pitfalls and allows research in different aspects and models including multi-lingual ones.
** Using this, https://github.com/google-research/language/tree/master/language/canine for coding reference
* Second Research Paper: BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION
** This paper mentions the significance of machine comprehension for answering queries about a given context and how it is important to model complex interactions between context and the query.
** This paper was mainly about Bi-Directional Attention Flow (BIDAF - character-level, word-level, and contextual embeddings, and uses bi-directional attention flow to obtain a query-aware context representation) network, which is a multi-stage hierarchical process that shows the context at multiple levels of granularity and utilizes bidirectional attention flow mechanism to obtain a query-aware context representation without early summarization. This models evaluations match the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.
** Model:
** [[files/BIDAFModel.png | height = 250px]]
** The model very well explains the six different layers it has:
*** Character Embedding Layer
*** Word Embedding Layer
*** Contextual Embedding Layer
*** Attention Flow Layer
*** Modeling Layer
*** Output Layer
** Something that I found interesting was in Contextual Embedding layer, the use of Long Short-Term Memory Network(LSTM) with the embeddings provided by the previous layers to model the temporal interactions between words. Obtain H ∈ R 2d×T by placing the prior mentioned in both directions.
** Another intriguing part of the paper was learning about the different experiments held for understanding the model better. "SQuAD is a machine comprehension dataset on a large set of Wikipedia articles, with more than 100,000 questions". This is the dataset we are using for our test to get the following results:
** [[files/SecondPaperResults.png | height = 250px]]
** We can see the use of types of the tokenizers used and using 1D CNN char embedding.
** The result of the model are above and what I got from it is basically that this model successfully matches the Stanford identified results and are depicted tin the visualization below. This paper further made me understand how tokenizers and embeddings when working on top of one another can really help improve results but the CANINE model as seen above, tries to diminish that significance to provide efficient results even without tokenizers. Another experiment with the cloze test was done, but I found this one slightly more complex, but was able to compare it well with the required results.
** [[files/SecondPaperVisual.png | height = 250px]]
** Overall, this paper shows that the model is the correct depiction for Machine Comprehension and is able to answer complex questions by attending correct locations in the given paragraph.
* <b>Focus</b>
** First, figure out how the model I am trying to make into a primitive is imported as a module in python. (in a new file)
** Then, add the definition of the model (the way fork was done by previous members) in neural_network_methods.py
** and then finally for wrapping it into a primitive add it into gp_framework_helper.py
** Finally, then test the added primitive layer using Stand Alone Tree Evaluator
* Other updates:
** Added CANINE model code to EMADE.
** Figuring out how to convert it to a primitive. Asked Manas and Aditi and got some help, but it would be better to work with some older member of the team to gain a better understanding.
** Installed few required modules for CANINE model.
'''Sub-team meeting Notes:'''
* No meeting held this week as everyone had a time conflict. We are now scheduling a new time for the meetings.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 19th, 2022
|September 26th, 2022
|September 23rd, 2022
|-
|Research on CANINE and BIDAF
|Completed
|September 19th, 2022
|September 26th, 2022
|September 24th, 2022
|-
|Sub-team meeting
|Completed
|September 19th, 2022
|September 26th, 2022
|September 26th, 2022
|-
|Convert to primitive
|To do
|September 19th, 2022
|September 26th, 2022
|September 25th, 2022
|-
|Add CANINE as layer
|Completed
|September 19th, 2022
|September 26th, 2022
|September 25th, 2022
|}
== Week 4: September 12th - September 19th(2022) == 
===Outline of class notes===
'''In-class notes:'''
* Discussed more about yesterday's sub-team meeting as many of us couldn't attend in the ten minutes prior to the SCRUMs.
* Make sure to do self-evals (already completed)
* Have to add in some code work that I have to my notebook.
* Main wiki page has a guide to fix package installation issues for M1.
* Went over SCRUMs for the past two weeks.
* Tasks:
** Work on how to convert a hugging face transformer model into and EMADE primitive and work around with stand alone tree evaluator to put it into a tree and test
'''Individual Notes:'''
* Talked with an alum, Karthik Subramaniam.
** Go over Bidac paper.
** Create a primitives folder in EMADE for making a generic process to convert huggingface transformer models into EMADE primitives.
** Write your own class files and pass data from models into these.
** Should use stand alone tree evaluator as a unit test type run to make sure the generic file for embedding such models into EMADE is working.
** Make config file and pass that as command line  (seeded architecture). Master.out will be used as starting point.
** In your primitive have a file called huggingface file for methods with multiple primitives you want. Result to be an object that should be passed in and run the evolutionary algorithm to test
** Import the huggingface model. Get an object to be passed through 
** Huggingfaceprimitivename.yml that helps it convert and parse and then call helper to get an object to build your config file and then pass into PACE. 
** Scp it over to PACE to have an automated workflow
** Make huggingface more integrated.
* Tian and I then decided to start looking over at difference huggingface transformer models. We finalized CANINE to first convert it into an EMADE primitive and then hopefully make this conversion process more generalized.
* This week I have been reading two main papers:
** https://arxiv.org/abs/2103.06874
** https://arxiv.org/pdf/1611.01603.pdf
* I will give a full summarized explanation of the two in my notebook once I complete them. I did a decent amount of research with Tian and Karthik to understand how this could work and what files, such as qa_methods.py and qa_primitives.py that we could take reference from.
* I wrote some code on my local to run CANINE and get set up with huggingface on my local machine by installing it using pip.
    * from transformers import CanineTokenizer, CanineModel
    * model = CanineModel.from_pretrained('google/canine-s')
    * tokenizer = CanineTokenizer.from_pretrained('google/canine-s')

    * inputs = ["Life is like a box of chocolates.", "You never know what you gonna get."]
    * encoding = tokenizer(inputs, padding="longest", truncation=True, return_tensors="pt")

    * outputs = model(**encoding) # forward pass
    * pooled_output = outputs.pooler_output
    * sequence_output = outputs.last_hidden_state 
'''Sub-team meeting notes:'''
* I talked with Karthik mostly during this time.
* Updates for embeddings will now be different.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 12th, 2022
|September 19th, 2022
|September 14th, 2022
|-
|Research on CANINE and hugginface
|Completed
|September 12th, 2022
|September 19th, 2022
|September 18th, 2022
|-
|Sub-team meeting
|Completed 
|September 12th, 2022
|September 19th, 2022
|September 14th, 2022
|-
|Write code for conversion to EMADE primitives
|To do
|September 12th, 2022
|September 19th, 2022
|September 14th, 2022
|}
== Week 3: September 5th - September 12th(2022) == 
===Outline of class notes===
'''In-class notes:'''
* No class due to Labor day
'''Self-Evaluation Form:'''
* https://drive.google.com/file/d/1YKoSziCZWpoi8QXnY7S3xmjwRHPotpHx/view?usp=sharing
'''Sub-team meeting notes:'''
* Split the team into two breakout rooms
* I am a part of the Embeddings sub teams
* We went over all the research we did, especially new members, over the past week.
* We have decided to first get an understanding on how implement some simple embeddings like Glove or Word2Vec and go over EMADE implementation for DistilBERT to understand how to write code specific for EMADE to add embeddings
* Will try to meet once more with some ready code.
* Tasks:
** Go over https://keras.io/examples/nlp/pretrained_word_embeddings/ and https://nlp.stanford.edu/projects/glove/ with Manas's python notebook in the emade folder
'''Sub-team meeting #2:'''
* Another quick meeting was held at 8pm on Sunday for updates. I could not attend it as I had a prior meeting commitment. They discussed basic embeddings that they implemented for trail over the week.
* I will post updates on the channel.
'''Individual Notes:'''
* Downloaded gloVe dataset.
* Made some minor code changes in files to run the embeddings and see test results
* I was facing an error, which is apparently specific to M1 chips on macs, so I will try to figure that out.
* The demo.sh runs for GloVe aren't really required and running Manas's python notebook worked to get glorified embeddings results. Will try to look deeper into DistilBERT more to understand if we require to decrease the amount of hardcoding.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 5th, 2022
|September 12th, 2022
|September 11th, 2022
|-
|Research on Embeddings
|Completed
|September 5th, 2022
|September 12th, 2022
|September 10th, 2022
|-
|Sub-team meeting
|Completed 
|September 5th, 2022
|September 12th, 2022
|September 7th & 11th, 2022
|-
|Running Manas's notebook on GloVe
|Completed
|September 5th, 2022
|September 12th, 2022
|September 10th, 2022
|}
== Week 2: August 29th - September 5th(2022) == 
===Outline of class notes===
'''In-class notes:'''
* Went over SCRUMs
* Work on errored individuals for NLP. General evolutionary improvement for NLP tasks is a goal we can look into.
* Integrating more transformers from huggingface into EMADE seems like a good idea.
* One subteam for huggingface stuff and the other for evolutionary improvements maybe.
* Assign tasks for this week
* Discussed work of other subteams, mentioned in the Sub-team weekly reports. Found tasks for image processing interesting considering how they are using stand alone evaluators and checking for overfitting.
* Our number one project choice was continuing with the NLP Q/A and the second project options are huggingface transformers (use another embedding), chatbots, text summarization, and topic classification. Probably going ahead with transformers. The third one which we may start and pass over for next semester work is text summarization or chatbot.
* For text summarization, it is a bit tricky to do supervised learning as we don't have a good validation set, which is why we can focus on looking for a good objective function to put into EMADE.
Tasks:
* I have to work on embeddings.
'''Sub-team meeting notes:'''
* Discussed how the nlp-qa teams will work on tweaking the mating mutation functions and other code to get better results and the embeddings teams can use this link, https://keras.io/examples/nlp/text_extraction_with_bert/, as reference and work on adding these.
* Last semester embedding was BERT.
* I got to get set up with EMADE again as I had not done it last semester, but had done it last to last semester.
* Q/A: 
** Taking a look at the xml.
** Changing mating and mutation functions by researching into more python libraries.
** Check similarities with previous results and make some other changes (hit-and-trial)
** Adding new functions or messing with hyperparameters.
** Look for some research papers.
** https://github.gatech.edu/sleone6/emade/blob/nlp-qa/src/GPFramework/neural_network_methods.py location with NNLearner primitives.
* Embeddings:
** Implementing different huggingface models and embeddings into primitives.
** Decide which one and how many
** Start with and embedding that was done before to get used to the procedure. (GTP)
** Glove, word-to-vec, GTP, BERT (other names)
** Research on embeddings.
'''Individual Notes:'''
* Started getting setup on PACE. 
* Had to delete some older stuff I had on PACE since I used it for image processing earlier and started getting storage quota errors.
* Most of the stuff worked successfully but I am getting the following error right now: 'ERROR: Could not find a version that satisfies the requirement gpframework==1.0'
'ERROR: No matching distribution found for gpframework==1.0'
* Did some research on what embeddings are from the documents sent by Manas.
* Figured out the GPFramework issue and have got email successfully set up on Pace now. Turns out installing it directly on an env was giving errors so I used the reinstall.sh script to do it and it worked.
* Research:
** https://docs.google.com/presentation/d/1eC_b0PkMGHdjzzNgQu0rMPFOpS_nK689/edit#slide=id.p1: This presentation by Devan was a super helpful resource to understand a lot about embeddings in the field of Natural Language Processing. Understood more about different techniques of converting strings to numeric relational values such as Euclidean distance and Cosine Similarity. I learned more about layering and what the use of embeddings will be in EMADE.
** https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa: Through this I was able to comprehend everything mentioned in the previous presentation better too and learned about an easier embedding process of Word2Vec using the Skip-Gram model on the website.
** https://docs.google.com/presentation/d/15AgY4IFxTOf4xcTtqm-DeCRZ_IY69Kacf2LReI5b1hQ/edit#slide=id.g118abeb7cda_2_21: Went over last semester's presentation to gain some knowledge on DistilBERT and BERT, the embedding that MLP actually implemented on EMADE in the previous semester. It uses a distillation process to reduce number of parameters and retain the important details for better and quicker results.
** https://medium.com/analytics-vidhya/introduction-to-word-embeddings-c2ba135dce2f: This was also more of a basic understanding webpage to understanding two basic embeddings of Word2Vec and GloVe.
** https://keras.io/examples/nlp/text_extraction_with_bert/: Used this keras API webpage to go over the way the code was implemented by the previous NLP onto EMADE.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|August 29th, 2022
|September 5th, 2022
|August 31st, 2022
|-
|Research on Embeddings
|To do
|August 29th, 2022
|September 5th, 2022
|August 31st, 2022
|-
|Sub-team meeting
|Completed 
|August 29th, 2022
|September 5th, 2022
|August 31st, 2022
|-
|EMADE Set up
|Completed
|August 29th, 2022
|September 5th, 2022
|September 5th, 2022
|}
== Week 1: August 22nd - August 29th(2022) == 
===Outline of class notes===
'''In-class notes:'''
* Discussed summer updates
* There were two publications made through this VIP. One of them was the Stocks team poster presentation in the Gecco conference in Boston. Was received pretty well and seems like an interesting topic to pick up this semester.
* The other one was a publication on "Evolving SimGANs to improve abnormal electrocardiogram classification"
* Eric talked about scoliosis measurements update that him and GTRI worked on. Realized over the summer that input data of VerSe dataset was different from Shriner's dataset.
* They looked for papers with datasets similar to Shriner's dataset and were able to replicate results of the new paper themselves.
* Objective functions are done, which means Cobb angle can be calculated.
* Good progress made in segmentation of images. Interested in seeing the future of this team, which I could possibly be still working on.
* Getting Spring 2022 updates of all teams.
** As of now, the Stocks team sounds exciting with the time series issues and NLP also sounds challenging with the multiple issues they mentioned such as tokenization, improving mutation and mating functions, dimensionality etc.
** Image processing team looks to improve on the results they got at the end of last semester as they got bad results on the ChexNet dataset. Used a lot of NAS code. Mentioned bounding box issues as of now.
** NAS shared a lot of common code with image processing and were focussed on modularity. Looking into co-evolution of some modules.
** In scoliosis, finally a single model works so we can start with genetic programming using EMADE. Will incorporate some of the NAS components.
* I really like what the Scoliosis team is doing, but I have always been interested in NLP so I want to try learn more about it this semester so chose that sub-team.
* New topics are being introduced:
** NAS: Co-evolution, modularity
** Many Objective options (EMADE), Island models, Quality diversity
* Going to do one brainstorming session to figure out the issue with NLP to hopefully keep going forward with some intriguing research, maybe Q/A or something new.
'''Sub-team meeting notes:'''
* Went over NLP's last semester final presentation to understand the latest progress and work on it or develop something new.
* Discussed three possibilities for this semester. Will think over it and have a vote by the next VIP meeting. (Link to Google doc with ideas: https://docs.google.com/document/d/1Zx8pxWm5JjUh3bwg8J-EwdcT87o7KWUEctwbgJJ1LSo/edit) 
* Will go over code base and add new ideas. Chatbot idea seems very interesting to me
'''Individual Notes:'''
* Went over the google docs above and did some research on the kind of projects that I am interested in working on while in the NLP team. I found the summarizer and translator super interesting.
* We all have provided our preferences through a Qualtrics survey and will now meet up in the regular meeting to discuss, which one to take on.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|August 22nd, 2022
|August 29th, 2022
|August 22nd, 2022
|-
|Team Selection
|Completed
|August 22nd, 2022
|August 29th, 2022
|August 22nd, 2022
|-
|Sub-team meeting
|Completed 
|August 22nd, 2022
|August 29th, 2022
|August 24th, 2022
|}
= Spring 2022 = 
== Final Presentation Day== 
===Outline of class notes===
* Our final presentation link: https://docs.google.com/presentation/d/1ZwgN-X2JhJYEB0J7rJAa4NUwpJGaTAQnhmArzzt6d3g/edit?usp=sharing
'''Natural Language Processing'''
* Explained what NLP is first and explained how Q/A works in NLP. This is something they always mention, which I find quite nice.
* They then talked about what they worked on to improve Q/A using NLP. They made their NNLearnerQA.
* It takes in two input IDs. Working: Input IDs -> Embedding -> Any structure of neural network layers -> Softmax output
* Talked about improving Q/A by using AutoML and mentioned the results from last semester.
* This semester they worked on giving EMADE the ability to work on replicating an existing state of the art model.
* They mentioned revamping the embeddings this semester and how they used a Fork primitive to convert a non tree based Keras API model to a tree based structure.
* They ran into some memory usage errors and mentioned how they use the BERT model for this project (bidirectional self attention mechanism). To fix the memory issue they switched to a transformer primitive method, which was 5-6 times faster. They later realized that the memory error was their own fault; however as the transformer primitive method was faster it was a net positive.
* Modules downloaded from huggingface.co, a website that even I like a lot.
* Explained their tokenization (splitting sentences into tokens to make it easier for using NLP to understand context) problem : pass words itself instead of indices.
* they didn't get any valid results due to some EMADE issue, but when ran manually, they got an individual that reduced the AUC by 65%
* Future work is to debug deep ensemble issue, tokenization problem and work on mutation and mating methods.
* Overall a very nice, informative, and organized presentation.

'''Neural Architecture Search'''
* Implementing functionality to work on and process the Amazon dataset.
* They explained the main need of the Amazon dataset and then mentioned how in their current work they were getting some Cryptography packet error.
* They had to make many manual changes to convert image data to text data.
* They mentioned that they removed some convolutional layer instances -- were getting dimensionality errors.
* They indicated the need for creating custom seeding models and have done the initial validation.
* I found it quite exciting how they are working on creating such custom models; however, I am still a little confused on some of the working methodologies used in NAS.
* They proposed use of new metrics and mentioned how some work for NSGA II but not lexicase. That could be something they work on later.
* They need to work on some installation modifications and improving visualization of individuals along with fitness adjustments.

'''Scoliosis measurements'''
* Our presentation went pretty smoothly. The members from Schriner's hospital also really liked the work we have done. I am pretty excited for the future of the project and am really enjoying working on this project.
* Cole asked us a few things about whether we are labelling autonomously or manually and how the process work, which we mentioned was autonomous using the Azure labelling tool currently. Someone in the class suggested that we could check more individual vertebrae to compare the angle with the technicians measured angle.
* Dr. Zutty mentioned how we could compare our model with the Azure labelling tool, but turns out the Azure labelling tool takes a much longer time. However, this could be a good area of research for the future as well. We discussed a few more interesting points around this and the Dr. Rohling also mentioned that if we got an angle of 48 when the technician got 42, maybe we did get the right angle but got (90-the angle) instead of the angle itself? That seems unlikely currently but could be. Something we will look into. As of now though, it is still better to get a higher angle for overfitting and cautionary measures.

'''Image Processing'''
* They have modeled their work around the CheXNet paper. Largest public dataset for chestxrays.
* They then talked about their evaluation method -- use of ROC per class instead of average (15 total)
* Mentioned how they improved in the segment of data preprocessing (used full dataset now) from the midterm (had less than 20 instances of some classes).
* New result values were close to 0.5, which is not a good result at all as that is basically randomly guessing. According to the team this could be because of error in the labelling files, not including augmented images, and/or data corruption.
* As the standalone evaluation was also bad, according to them the good tree which gave 0.2 (1-AUCROC) will probably give bad results on the new dataset like the bad tree.
* Decided to use ADF Tree Parser and rewrote scripts in order to fix bugs.
* They made a lot of edge case modifications to make improvements.
* Much more organized tree/visualization attained
* They then depicted result comparisons between mid term and finals and lexicase and NSGA II.
* Lexicase seems to be best selection method but still runs really slow.
* They still need to run NSGA III as they are getting many errors.
* Overall, it was really interesting seeing how far the team has come since I was also a part of this team last semester, especially being able to work more with lexicase. I am excited to see what goes on next.
'''Note:'''  My notes for other teams' presentations may not be too comprehensive as sometimes I felt I needed to pay attention to how they were presenting results and what they were talking about to understand their methods and concepts due to which I was unable to take notes at those times.
<br><br>
'''Final remarks:'''
I really enjoyed all the work I did this semester and was glad I was able to make some positive contributions to my sub-team. It was very exciting to work with Schriner's and learn a lot more about ML in healthcare. I look forward to working on this project and am willing to contribute over the summer as well to make sure I stay in the loop with whatever new modifications are being made. I hope to get a paper ready too for the DMAH conference. I was able to take more responsibility in terms of workload, managing members, and contributions in the team this semester as I believe I had understood the project concept well and learned a lot from my time in the VIP. It was a great experience and I definitely gained a lot of knowledge doing research and from my sub-team members.

== Week 15: April 25th - April 29th(2022) == 
===Outline of class notes===
'''In-class notes:'''
* Final presentation coming up on Friday
* I have completed Peer-Evals
* Discussing how we can complete the presentation and hopefully we can include all results and analysis this time.
* Started then with SCRUMS
'''Individual Notes:'''
* I added link to all files that I worked on, including the ones before the mid term like:
** Moving nii files: https://drive.google.com/file/d/10Lj2sLa-hNMZgvTvmXoa_mELzEBceaPk/view?usp=sharing 
** Mounting data: https://drive.google.com/file/d/1mUZ3RvCBK3x3lEiLNussiVe3J7BS7eVI/view?usp=sharing
** Running training script: https://drive.google.com/file/d/1hlyIEwwAGZrbZ-UTXyl3Jh52V5fKVDn6/view?usp=sharing
** I talked with Aaron and Anwesha about a few ideas for the cobb angle code: https://drive.google.com/file/d/1DMRaWZWU8SznjBmsOZpVNUVii1E2BzGw/view?usp=sharing -- Cobb angle evaluation script and https://drive.google.com/file/d/1Z0d5lCIgvb2YOvb2oXTjACCwRJOI0hUc/view?usp=sharing -- slope calculation techniques using bounding box
The working of this code is mentioned in the final presentation, which I will be linking in my notebook. This code is going to be working on the JSON files that we have with VerSe, which is similar as we have JSON files linked with DICOOM too.
** DICOM to PNG conversion with Aaron: https://drive.google.com/file/d/1FIHSKlNNth0bdpEOp-zPUzTxPxj9JGiZ/view?usp=sharing
** Currently working on script to convert DICOM to nii files for the new model
*** Finished writing the script to first move all DICOM images from multiple folders to one singular folder. This was one of the most important tasks for this conversion as there are a few functions that require all files to be in one directory while nii conversion. The way I did this was by using the shutil module and few basic pythonic functions to go into each individual folder and extract a DICOM image from each of them.
*** Links to be used for help for the rest: https://people.cas.sc.edu/rorden/mricron/dcm2nii.html & https://nipype.readthedocs.io/en/latest/api/generated/nipype.interfaces.dcm2nii.html 
& https://pycad.co/how-to-convert-a-dicom-series-into-one-nifti-file-python/
*** Now we just need to use the right method for conversion, which one of us should be able to do soon.
*** Script I wrote for file movement: https://drive.google.com/file/d/16FvZS7sGA7p58Uv9FYCuRHuzmOB6W09H/view?usp=sharing
* Correct to-dos that I am a part of for the future of this project: 
** We need to be able turn the model output into a format similar to what the input for that code should look like. May be able to find out the format by looking through the code we will also need to figure out how to get predictions for an arbitrary image, in figuring this out 
** May be able to find out what the output is formatted like. We need to annotate images to show what lines the model is using to make its cobb angle prediction. This is helpful for the paper, but it will also help us tell if/why the cobb angles are off.
** The code for DICOM to nii conversion isn't going to be hard to generate, which we can focus on to get the model running on Schriner's dataset.
'''Sub-team meeting notes:'''
* Unfortunately I couldn't attend the meeting on Thursday as I had a final, but we mainly decided who is going to talk about which slides on the final presentation. I will be focussing on the slide about the X-ray datasets.
* Updates are mentioned on the final presentation. We got he cobb evaluation code ready, but we are still going try working to get a paper ready by May 10.
* Current tasks for everyone before the final presentation:
** Dicom Conversion - Rohan, Brian, Sriram, Anish
** Manually Label Shriners - Anwesha, Ruchi, Nikil, Harris, Patrick
** Job Submission .yaml file - Nathan, Eric
** Metadata Script extraction - Tian, Prahlad
** Visualizing Cobb Angle Lines - Anwesha, Harris
*Current challenges and blockers we have discussed:
** Getting errors when performing vertebrae localization. 
** Difficulty adapting the models post-processing code to display heat maps as png’s.
** Not clear how to adapt the model to be able to perform segmentation on Shriner’s images. May be as easy as converting dicom files .nii.gz (which is almost done)
** Once these are figured out, cobb angle measurements become simple
* We have almost completed the final presentation and the cobb evaluation code that we worked on is well explained on the slides to which i will be providing a link.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|April 25th, 2022
|April 29th, 2022
|April 28th, 2022
|-
|Try running models
|Completed
|April 25th, 2022
|April 29th, 2022
|April 28th, 2022
|-
|Complete cobb angle evaluation code 
|Completed
|April 25th, 2022
|April 29th, 2022
|April 27th, 2022
|-
|Final Presentation slides
|Completed
|April 25th, 2022
|April 29th, 2022
|April 29th, 2022
|-
|Write conversion scripts 
|Completed
|April 25th, 2022
|April 29th, 2022
|April 26th, 2022
|-
|Sub-team meeting
|Completed
|April 25th, 2022
|April 29th, 2022
|April 28th, 2022
|}
== Week 14: April 18th - April 25th(2022) == 
===Outline of class notes===
'''In-class notes:'''
* Spent the first ten minutes discussing whether or not we should write a paper for our research. We are still a little skeptical on as to how  to go about it with finals coming and our model not having produced results yet, which is why we have scheduled a meeting with the Schriner's hospital for this weekend. 
* I also explained to the new students how we are trying to integrate two different papers and what both the papers talk about. Harris mentioned that we may be using a wrong VerSe dataset and may have to fix that too, which could make our tasks easier.
* All tasks that were assigned last week remain.
'''Friday Short Paper meeting notes:'''
* Joined a call at 3:30 pm with Schriner's hospital and the other members of our team.
* Will discuss whether we can write a short paper for  International Workshop on Data Management and Analytics for Medicine and Healthcare conference. They look for more tactical and clinical submissions usually and not necessarily overly technical submissions.
* From a healthcare conference perspective it is technical, but from a CS and data science perspective it may not be too technical.
* Our goal is to get a prototype ready for the model that can run on the VerSe dataset and then work with the Schriner's dataset.
* Whether or not the results our good we will continue to work on this over the summer and if possible we can try to submit the short paper by 10th may and if not then extend the deadline to complete over summer.
* If we have some results we can write the paper that could be sent for the DMAH conference.
* Personally, I am pretty excited about this and although I have an internship, I would love to work and contribute towards this over the summer.
'''Individual Notes:'''
* To run the model now, the github_repo_attempt user folder has the right files and updated files for the same. No need to run the server loop.
*Changes now that have been made are basically -- corrected files locations and corrected VerSe dataset.
* run_verse_1.yaml to submit jobs. The command to begin submitting a job is mentioned in the user folder. Link to file: https://drive.google.com/file/d/1YlUDoGTbFTYDtRkJklm5DvJpY72Gu3Nl/view?usp=sharing
* To do: Complete cobb angle code
* I worked on developing a new folder where all the work for the cobb angle evaluation code will be stored.
* Used simple command like cp -a /source/. dest/ to copy over files and codes from one folder to another.
* Now that we know how segmentation is working, developing cobb angle eval code should be easier, which I will be doing along with three other members.
'''Subteam meeting:'''
* Discussed how we can finally start running the model. All steps are given in the github_repo_attempt user folder.
* Now that the model is running successfully, we need to start working on our presentation.
* Comparing every single pair of the part of the vertebrae to find out the highest angle.
* Dataset is correctly stored in datasets folder under verse2020new
* Using the mid point method to measure should be the easiest way of doing this, which is what we will go further with most probably.
* We will work on the cobb eval code in the github repo folder.
* Image depicting our ideology on as to how we are working on evaluating the cobb angle:
**[[files/drawing.png | height = 100px]]
* The code is almost ready.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|April 18th, 2022
|April 25th, 2022
|April 24th, 2022
|-
|Work on cobb evaluation code
|Completed on Monday
|April 18th, 2022
|April 25th, 2022
|April 25th, 2022
|-
|Set up the correct VerSe dataset 
|Completed
|April 18th, 2022
|April 25th, 2022
|April 24th, 2022
|-
|Run model locally
|Completed
|April 18th, 2022
|April 25th, 2022
|April 23th, 2022
|-
|Sub-team meeting
|Completed
|April 18th, 2022
|April 25th, 2022
|April 24th, 2022
|}
== Week 13: April 11th - April 18th(2022) == 
===Outline of class notes===
'''In-class notes:'''
* Spent the first ten minutes in trying to figure out if anyone was able to fix the errors most of us were facing in yesterday's sub team meeting.
* Turns out Nathan and Tian were able to get their runs to work without crashing but as a run takes about 40 hrs, they weren't able to finish one entirely.
* Then, we went over sub-teams SCRUMs.
* Everyone has already been assigned tasks for this week, which are mentioned in last week's notes.
'''Individual Notes:'''
* After and during class I was very eager to figure out why all of our runs were crashing and Nathan's and Tian's wasn't. So, I started from scratch.
* I deleted all the folders in my user folder and duplicated and copied over everything from Nathan's user folder. 
* I then made all required changes to every script in accordance with my user folder and then set up the environment for python 3.7.13 on a new compute node.
* Once this was done, I ran the serverloop script (after installing the required modules) and then ran the training scripts on a separate terminal window.
* My server_loop script did give me a SimpleITK error but did not crash and turns out that kind of error is common for serverloop and shouldn't matter much. 
* Surprisingly, the training scripts did not crash this time and this is how I believe that the error was fixed. Maybe, when Harris and I had been working on the VerSe dataset for mounting scripts and moving files around, we may have made some unforseen changes, which now seemed to be working after just starting from scratch.
* Now, for my other task of the week, I am going to try to code the scripts for cobb angle evaluation. I first cloned the vertebrae landmark detection model into my Azure user folder.
* I am still waiting for the labelled data from the other sub-team that is working on that in our group and I am currently working on how to use nii files instead of jpg images to run this script.  
* Currently, after talking to Harris and Aaron we decided the modifying the scripts may be more complicated than writing it from scratch itself. So we may try that next. Some changes may have to be made in the VerSe dataset, which is what we figured after performing more research online as this will help in removing the current file discrepancies issues we are facing.
* I also tried to run the VerSe model as well, but lost wifi connection which stalled the run. We are trying to fix job issues and SimpleITK errors to ensure the run doesn't get stalled that easily.
* I also tried running the Landmark detection model locally on different jpg images and now that we have some idea of how the cobb evaluation works, Me and Sriram are going to try and combine the two papers to work on cobb evaluation on VerSe. Aaron is also done working on the job submissions, which should make our work easier too.
'''Sub-team meeting'''
* No formal sub-team meeting due to Easter.
* We, however, updated one another with whatever work we did.
* All updates for this week are here: https://docs.google.com/document/d/1EAWfHtWUIfCI4XOBV42B_ybislx-H4WUrXfUvQbicpc/edit?usp=sharing
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|April 11th, 2022
|April 18th, 2022
|April 17th, 2022
|-
|Run the model on Azure
|No errors but tough to run model for 40 hours straight
|April 11th, 2022
|April 18th, 2022
|April 17th, 2022
|-
|Help new students with set up
|Completed
|April 11th, 2022
|April 18th, 2022
|April 17th, 2022
|-
|Set up the landmark detection model 
|Completed
|April 11th, 2022
|April 18th, 2022
|April 15th, 2022
|-
|Run model locally
|Completed
|April 11th, 2022
|April 18th, 2022
|April 17th, 2022
|-
|Sub-team meeting
|Completed
|April 11th, 2022
|April 18th, 2022
|April 17th, 2022
|}
== Week 12: April 4th - April 11th(2022) == 
===Outline of class notes===
'''In class notes:'''
* Spent the first ten minutes on going over everyones progress, helping new students, and checking if Tian's run is still going (which it still has been going on since yesterday)
* Then went over sub-teams SCRUMs
* Will have Dr.Zutty come over to discuss the with us the issues we are going through and for understanding more about the model run time.
* Everyone this week will try running the model and if Tian's run is successful then we can set up our environments in a similar way.
'''Indvidual Notes:'''
* This week I first set up the environment as suggested by Tian.
* Initially I was facing a few environment issues set up. The SimpleITK module was unable to be installed whenever I tried to do so in my conda environment, but then finally after doing some research on Google I saw that I required a setupModule too. I conda and pip installed the same.
* When I tried running the model then I faced some pythonic errors then. Turns out the conda environment didn't seem to work correctly and we figured that the error was because python 3.8.1 was unsuccessful when it comes to running this model. 
* We then tried with python 3.7.13, which seemed to work somewhat at least for the initial parts of the runs
* The few modifications I made were basically just changing the Python version to 3.7 in my new conda environment and correlating the right tensorflow gpu version and simpleITK module versions.
* The following are the errors that we all have been getting.
[[files/Python.png | height=350px]] [[files/SimpleITK.png | height=350px]] 
* I made a few path modifications in the training script along with Aaron by using python -m pdb, but still haven't exactly figured out where we are going wrong. I will be working on this for the rest of the week. This week got a little hectic due to mid-terms and assignments, but I am still trying to work on running a successful run for the model.
* I have already got PACE set up, which I will now be helping the new students do so that we can start running the model on PACE with EMADE.
* Note: to configure the environment.yml file and set up an environment in my directory I ran the following command: conda env create -f environment.yml
* I did some research by using the following link, https://docs.microsoft.com/en-us/cli/azure/ml/job?view=azure-cli-latest#az-ml-job-create, on job creation and submitting jobs to help us in running models more easily on Azure. Currently, I haven't been to successful in finding out how this will exactly work, but this was another task that I had started working on to improve team efficiency.
'''Sub-team meeting Notes:'''
* We discussed what everyone had worked on and turns out almost everyone had environment set up problems.
* This was more of a work session where we tried helping each other fix errors. Everyone, after fixing all version errors, are currently mainly facing the SimpleITK error. 
* We then decided our tasks for next week instead of everyone just trying to run the model. 
* For next week, I will still continue my research on how to run these models and ask Tian for any set up help that he may have gone through. I will also work on developing code for Cobb angle measurement. This is one of the most important tasks that we need to complete soon.
* The others have also been assigned tasks like setting up PACE, running models, labelling images, etc.
* Hopefully, next week we make some progress on these tasks.
* The goals of the entire team in general for the final presentation are to get the model running and write code for cobb angle measurement.
* As for this week, the tasks for rest of the team are:
** Run the model in Tians Environment (Nikhil, Nathan, Harris)
Two main errors occurring with this
RuntimeError: Exception thrown in SimpleITK ReadImage: ../..Code/IO/src/sitkImageReaderBase.cxx:99:
sitk::ERROR: The file “../../verse2020/verse2020_dataset/images_reoriented/verse403_verse_208_CT-sag.nii.gx”
tensorflow.python.framework.errors_impl.InvalidArgumentError: function_node__inference_train_step_8121 conv3DBackpropInputOpV2 only supports NDHWC on the CPU. 
node PartitionedCall/gradients/unet/sequential/prediction/conv3D_grad/Conv3DBackpropInputV2
Both Tian and Nathan aren't running into any of these.
** Look into Job Submission on Azure (Harris, Eric, Brian)
Eric and Brian are going to look into Job Submission capability on Azure and attempt to submit a simple job -- writing a txt file maybe.
Harris will join them in this task and will also look into getting the model running on PACE-ICE. 
** Begin Developing Code to take Cobb Angle Measurements (Me, Sriram, Anwesha, Anish, Tian)
We looked into a paper that had developed a model to take cobb angle measurements, and it may be worth looking into how they did it for some insight into how to go about it. Code as well as a link to the paper here: https://github.com/rohanbatra4/Vertebra-Landmark-Detection 
** Labeling/Setting up Dataset from Shriners (Patrick, Ruchi)
The X-ray pngs are saved toin Aaron's user folder at: Users/amcdaniel39/data/xrays_png
Aaron has looked into setting up datasets from data already in Azure here. If the azure ML dataset command gives errors try setting up the v2 CLI tools for azure.
Some info on setting up a dataset for labeling images: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-image-labeling-projects#create-a-dataset-from-uploaded-data
** Get set up on PACE-ICE (anyone who is not already set up on PACE-ICE)
This a sort of a low priority task that if it seems like it’ll take you significant effort or time, focus on your other task.
Youtube video showing someone setting up PACE-ICE: https://www.youtube.com/watch?v=LashYCCJF3E
There are scripts to automate this process, which can be found here: https://github.gatech.edu/cwhaley9/emade/tree/nn-vip with instructions here:https://github.gatech.edu/cwhaley9/emade/wiki/Running-EMADE-on-PACE-using-CIFAR-10
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|April 4th, 2022
|April 11th, 2022
|April 9th, 2022
|-
|Run the model on Azure
|Few errors currently
|April 4th, 2022
|April 11th, 2022
|April 11th, 2022
|-
|Help new students with set up
|Completed
|April 4th, 2022
|April 11th, 2022
|April 10th, 2022
|-
|Set up python 3.7 conda
|Completed
|April 4th, 2022
|April 11th, 2022
|April 10th, 2022
|-
|Sub-teaam meeting
|Completed
|April 4th, 2022
|April 11th, 2022
|April 10th, 2022
|}
== Week 11: March 28th - April 4th(2022) == 
===Outline of class notes===
'''In class notes:'''
* Back from spring break. Now, it's time to start working on running the model and then start development with EMADE.
* Introductions with the Bootcamp students.
* Gave all of them access to the google drive with the meeting notes.
* Will email the hospital to get them access to Azure
* The current tasks for the older students is to get the model running and the task for the new students is to go over our notes and read the paper based on which our model revolves around currently. Hopefully, they can get access to Azure quickly so that they can start helping us with the tasks.
'''Individual Notes:'''
* This week we are trying to work over how to run the training scripts without any errors.
* While going over the scripts I decided to change the mixed_precision variable to false as that was the main error we were getting.
* Initially we were getting an empty tensorflow container error, but now Harris got a GPU error and I got a missing files error which was slightly odd, but I am still going over running it and changing a few things here and there.
* Checking the mixed_precision document, Harris mentioned that the compute capability required for the mixed_precesion is 7.3 and ours currently is lesser, so we might ask the hospital for a higher capability GPU. 
* If that doesn't work out, we might try and just work without mixed_precision.
* Surprisingly, I had tried using a conda environment earlier, which is why the only errors I had were file reader version errors, whereas others were getting tensorflow errors. now that Tian tried a different kind of environment, so it seems like conda was the way to go and we may have been facing just some environment problems. These were some of commands that I was going through for conda, which is what Tian also went through then to make changes to conda environments.
** conda create -n verse python=3.8
** conda install tensorflow-gpu==2.2.1 scikit-image==0.17.2 tqdm==4.51.0 networkx==2.5 lz4 zfpy
** conda install -c conda-forge Pyro4==4.80 itk==5.1.0
** conda install -c simpleitk SimpleITK==1.2.4
* Aaron taught me the usage of glob and pdb in python, which was quite insightful to debug the file reader errors I was getting.
* Some of the commands include
** Using -m pdb before python scripts
** glob.glob("file_name") to search and go through files
** importing os for checking path existence.
'''Sub-team meeting notes:'''
* We first reintroduced ourselves with the new students.
* Harris and I are working on running the model and our discussing our errors with the team, so hopefully we can brainstorm and try to fix these errors.
* Helped new students set up a few things on Azure and get familiar with Schriner's portal.
* So, currently our main problem is:
** Fixing mixed_precision issue and I will mention the solution either below or in next weeks's notes.
** One solution that we currently found out that seems to be working for Tian is using a conda environment with certain specifications
* This is the environment file set up by Tian.
** https://drive.google.com/file/d/1Rm436g0JdhFFWgUxiZVBp9x5QF_UzsUh/view?usp=sharing
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|March 28th, 2022
|April 4th, 2022
|April 2nd, 2022
|-
|Run the model on Azure
|In progress
|March 28th, 2022
|April 4th, 2022
|April 4th, 2022
|-
|Learn error debugging on python scripts
|Completed
|March 28th, 2022
|April 4th, 2022
|April 3th, 2022
|-
|Sub-teaam meeting
|Completed
|March 28th, 2022
|April 4th, 2022
|April 3th, 2022
|}
== Week 10: March 14th - March 21st(2022) == 
===Outline of class notes===
'''Mid-term presentations:'''
* Unfortunately, I could only stay for the presentation by NLP as I had to leave before 6:30 due to my CS2110 lab. After NLP, we presented and our presentation went really well overall.
* Link to our presentation: https://docs.google.com/presentation/d/1QLNUeUK52w-_7pUd8uohbSPgvj4djIL1xIn9dHkkq2M/edit?usp=sharing
* '''NLP:'''
** They started with mentioning what NLP is in general, which I felt was quite nice for bootcamp students especially.
** They then mentioned the importance of Q&A and mentioned how it is a part of information retrieval as that is the main concept around which NLP is revolving. They described how using Machine Learning with EMADE can be useful to improve the Q&A systems.
** They then did a quick review of the last semester, mentioning BiDAF and displaying the results from the previous semester.
** I like how they analysed their previous errors with individual and seed performances (NNLearner2) and worked on those errors to get better results.
** They then talked about using the Keras API model, how their model works, and how they successfully replicated it onto Google Collab.
** They have a few things left to implement in EMADE such as embedding (using BERT) and End index.
** Embedding is basically a method in NLP to map words or phrases from vocabulary to vector of real numbers. Conversion of words to real number is vectorization.
** They then talked about few of the example techniques they are going over : GloVe, DistilBERT, Word2Vec
** First, they discussed what GloVe is. It is an unsupervised learning algorithm for getting embeddings for words from a corpus. They are not particularly using GloVe.
** Second, they talked about Word2Vec, tool for generating word vectors that a numerical representation of words.
** Since, Word2Vec did not seem optimal they also tried using FastText.
** They then talked about DistilBERT. I was quite interested to know more about this as I did not quite get it the first time they mentioned BERT. They talked about how it is an encoder that uses several layers each of which are performing multi-head attention to find relationships between words in context and query. DistilBERT in particular uses distillation to reduce the number of paraemeters.
** They mentioned their results then and the use of pretrained models. Their results beat their last semester ones but they get an error while using at least 10% of the dataset.
** They started to then talk about the other task they are working on this time: End Index
** They seeded a run with NNLearner2 and now their goal is to obtain both the start and end index.
** It was interesting to see their techniques of data preprocessing. One of the problem they faced was that the Keras API model is not a tree. So they decided to change their approach and selected 'Fork'.
** Towards the end they mentioned how they modified the Q&A system and their future work is building on the problems they are facing and enhancing results using the 'Fork' approach.
* No such action items for this week as of yet. We hope to run the VerSe Challenge Winner model as soon as possible.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes and mid term presentation notes
|Completed
|March 14th, 2022
|March 18th, 2022
|March 16th, 2022
|-
|Run the model on Azure
|In progress
|March 7th, 2022
|March 30th, 2022
|After mid-term
|-
|Complete mid-term presentation
|Completed
|March 14th, 2022
|March 14th, 2022
|March 14th, 2022
|}
== Week 9: March 7th - March 14th(2022) == 
===Outline of class notes===
'''In-class Notes:'''
* Begin with SCRUMS
* Discussed what all to do for the mid-term presentation.
* Worked on how we can run the model to get some results using the VerSe challenge winner model.
* We have finished all main setup steps. Hopefully, running the model doesn't lead to many problems.
'''Sub-team meeting Notes #1:'''
* In our first sub-team meeting of the week we discussed how we could run the model successfully.
* All set up work was done and we were just going over reorientation of images.
* We then went over our mid-term presentation. Harris decided to mainly go over the introduction and the end of the presentation. Tian decided to go over about what Azure is. Prahlad was assigned the task of going over the results after running the model and if we are unable to run the model, then he would go over the results of the different research papers that we are revolving most of our work around. I helped make the slides for the Landmark paper as I chose to read that paper and already had notes for it. This was assigned to be presented by Nikhil. And finally, I was going to go over the datasets that we will be using (and our current achievements with the Schriner's x-ray images), implementation of how our model is set up on Azure, and errors that we got in our process. 
* We decided to meet again before the meeting as now our presentation slides had been assigned already but we just had to make sure that we were able to run the model.
'''Individual Notes:''' 
* Before running the model, we all went through the README and Harris also suggested that we need to reorient the images, which we all also realised then. I helped Harris with the same as I was more familiar with handling mounted datasets and we need more compute nodes as reorientation took time. 
* After all training, testing, and validation files (nii) were transferred we then reoriented the images and tried running the model.
* Initially, Prahlad and Harris tried running the model and got the following error : TypeError: Failed to convert object of type class ‘tensorflow.python.training.experimental.loss_scale.DynamicLossScale’ to Tensor. Contents: DynamicLossScale(initial_loss_scale=32768.0, increment_period=1000, multiplier=2.0). Consider casting elements to a supported type.
* I then joined in running the model and we changed the mixed precision value from true to false to get rid of the error but got another conversion error: raise NotImplementedError("Cannot convert a symbolic Tensor ({}) to a numpy" NotImplementedError: Cannot convert a symbolic Tensor (meshgrid/Size:0) to a numpy array.
* Other than this I worked on completing the slides that were assigned to me.
'''Sub-team meeting Notes #2:'''
* In this sub-team meeting we further tried to run the model and fix the above errors.
* We figured that there are many different approaches to fix the issues, but as we have our mid-term presentation coming up, we decided to focus on making the presentation better without results. We added the results from the two papers we read and expanded more on the VerSe dataset.
* Everyone completed and went over their own slides.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|March 7th, 2022
|March 14th, 2022
|March 13th, 2022
|-
|Review lecture 
|Completed
|March 7th, 2022
|March 14th, 2022
|March 13th, 2022
|-
|Run the model on Azure
|In progress
|March 7th, 2022
|March 14th, 2022
|After mid-term
|-
|Write scripts
|Completed
|March 7th, 2022
|March 14th, 2022
|March 13th, 2022
|-
|Sub-team meeting #1
|Completed
|March 7th, 2022
|March 14th, 2022
|March 11th, 2022
|-
|Sub-team meeting #2
|Completed
|March 7th, 2022
|March 14th, 2022
|March 13th, 2022
|-
|Complete mid-term presentation
|Completed
|March 7th, 2022
|March 14th, 2022
|March 13th, 2022
|}
== Week 8: February 28th - March 7th(2022) == 
===Outline of class notes===
'''In-class Notes:'''
* Discussion for first ten minutes with sub-teams. Harris and I are now deciding to use the VerSe datasets annotation and raw images to run the model mentioned in last week's tasks.
* Begin with SCRUMS 
* Tasks for this week are mentioned in last week's notes
'''Individual Notes:'''
* Harris and I discussed quite a bit over Slack on how we could set up and run 'https://github.com/yijingru/Vertebra-Landmark-Detection' this model; however, due to too many unknowns and blockers, we decided to switch to running and working on the VerSe challenge winner model for vertebrae segmentation.
* So, initially we cloned the repository for this model (https://github.com/christianpayer/MedicalDataAugmentationTool-VerSe/tree/master/verse2020 & https://github.com/christianpayer/MedicalDataAugmentationTool) onto Azure. We decided to work on Azure (the platform given to us by Schriner's) mainly because of its compute power and also as we could easily edit scripts, run models, cloned repositories, and mount datasets.
* After we had cloned the repository onto Azure, we then added the dataset (VerSe 2020) on Azure and wrote python scripts for mounting the dataset on Azure. Mounting is very important for such large datasets and allows us to access the data easily. Link: https://drive.google.com/file/d/1mUZ3RvCBK3x3lEiLNussiVe3J7BS7eVI/view?usp=sharing
* After mounting, we had to move the nii files (Link: https://drive.google.com/file/d/10Lj2sLa-hNMZgvTvmXoa_mELzEBceaPk/view?usp=sharing) from the dataset to the actual MedicalAugmentationTool that we were using as our model. We had thought that this would be easy but while Harris and I tried writing this script we ran into a few path blockers and weren't able to access the derivatives folder (the folder from which we had to move the images).
* I, then, did some research on how mounting works figured out that when datasets are mounted, they may appear to be easy to access but actually aren't. So I cd'ed into the folder where the dataset was and used vim to open the folder. I finally found the actual 'mount' folder where it was stored then using vim and used that to retrieve the dataset.
* I used the shutil module in python for copying the dataset into the images folder in the MedicalAugmentationTool. Moving was not required and copying seemed to be a better route. 
* Using some simple python libraries and modules I was able to successfully write the script. Initially Harris and I thought of using glob.glob to especially extract only nii files but as that seemed to be too complicated I used just some pythonic logic and string concatenation to complete this.
'''Sub-team meeting Notes:'''
* We discussed how we could run the model.
* We talked about everyone's updates. Harris and I had mainly been working on setting up the model on Azure and the others were working on metadata extraction from the X-Ray images as we had already converted the DICOM images to png.
* We hadn't yet figured out how to go about moving the nii files by the sub-team meeting. Aaron had suggested to maybe download a local copy of the dataset, but as mention above Harris and I had been able to write a successful script for the same.
{| class='wikitable'
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|February 28th, 2022
|March 7th, 2022
|March 4th, 2022
|-
|Review lecture 
|Completed
|February 28th, 2022
|March 7th, 2022
|March 4th, 2022
|-
|Set up the model on Azure
|Completed
|February 28th, 2022
|March 7th, 2022
|March 4th, 2022
|-
|Write scripts
|Completed
|February 28th, 2022
|March 7th, 2022
|March 6th, 2022
|-
|Sub-team meeting
|Completed
|February 28th, 2022
|March 7th, 2022
|March 6th, 2022
|}
== Week 7: February 21st - February 28th(2022) == 
===Outline of class notes===
'''In-class Notes:'''
* First ten minutes of class are now assigned for discussion with our own sub-team.
* Peer reviews have opened.
* Begin with SCRUMS 
* We are already tasked for the week as mentioned in last week's notes.
'''Individual Notes:'''
* For this week, my main tasks were to try and get datasets on PACE and perform runs and also get familiar with Azure and pydicom.
* I did a lot of research on pydicom and Azure and found out how you could convert dicom images provided to us to jpg images. (
https://stackoverflow.com/questions/48185544/read-and-open-dicom-images-using-python was a helpful website)
* I faced a lot of errors while trying to change formats but understood the usage of the pydicom libraries too.
* The one thing I figured out later was that even after converting the DICOM images to png or jpeg formats we will require a lot of normalization to actually be able to view the images. This is what Aaron helped us with. Normalization was pretty important to achieve the right pixel density for the images. The only reason I knew that normalization was important was because after looking on the Internet as to how DICOM images get converted and discussing with my sub-team as well, I realised that we would get completely blank images due to irregular contrast and pixel values.
* As far as the PACE work, I did the following work:
** A lot of research on getting around the storage quota on PACE, understanding the usage of the VerSe dataset, and using it to perform ML runs.
** I first configured all files in EMADE and made sure I had debugged and updated al my xml files as well. I then performed an EMADE run on the same, which then worked successfully.
** To add the VerSe data, I only used a small subset of the entire dataset as VerSe raw data is about 12 GB, which would currently be way over the storage quota of PACE. I made training and test data folders then and scp-ed it into PACE.
** To try and add the datasets, I went through the chest_xray.xml file first to figure out where we were accessing the dataset from while I was in the image processing team. The datasets were linked to npz files, which I couldn't make changes to as of yet. I tried directly linking the VerSe neural imaging files to the xml to run EMADE but that did not work and ran for only about 3 minutes before ending.
** I, then, changed the stratify.py file in the toxicity dataset of emade to make it use the VerSe dataset as the training and testing data. This worked and EMADE has been running successfully; however, I am not sure whether EMADE is actually going through the newly added dataset or not. I will try figuring out this once the runs are completed. If the VerSe data was not read correctly, I will then try to add the new data to the other datasets on emade to see if that works. 
** I am planning on taking some advice from Harris in our sub-team meeting as he has more experience with the Image Processing repository and PACE. As I was only a part of the image processing team for a few weeks, I spent most of my time working with PACE, which is why I am very comfortable with using PACE, but have had to spend a while trying to understand all the image processing code that was done before I joined the team after completing my bootcamp. 
* Overall, although I may not have gotten the apt results, I believe I was able to understand a great deal of the working of PACE and the image-processing code and as I tried many different methods of adding the datasets I was able to at least eliminate many incorrect methods to figure out the right one eventually, which will be very useful in the future. I am still doing as much research and tests on the same.
* Few other tasks that I plan on going over involve going over the following github to convert the NIFTI files to jpg before using them as datasets rather than trying configure the xmls on PACE: https://github.com/FNNDSC/med2image. And as our xml uses npz files I can try converting the .nii files to npz formatting. 
* Finally, the last thing I can try working on is trying to replicate results of the following github for more knowledge on how to go forward with this project: https://github.com/yijingru/Vertebra-Landmark-Detection
*<b> NOTE: Most of this tasking that I did above and research and tests that I performed are for future use. Currently, our goal now has become to first try and run the models without EMADE on PACE on the new VerSe datasets. Harris is trying that and i am helping him with the same. I tried working with EMADE to get familiar with the configurations and allow us to work more efficiently in the future. Our goal now has changed slightly as initially we thought we could move to running models using EMADE pretty soon, but due to some code blockers, we are going to work more on running it without EMADE. I have already been trying this but will not focus on this more instead of research on trying to make it EMADE-able.</b>
'''Sub-team meeting notes:'''
* Discussed everyone's progress for the week.
* Aaron was able to convert the DICOM images to jpg/png format.
* Harris and I made some decent progress on figuring out methods to run models on PACE and the others worked on understanding Azure and pydicom
* <b> Tasking for next week: </b>
** Harris and i are going to go over 'https://github.com/yijingru/Vertebra-Landmark-Detection' and try to run this paper on PACE as after this week we have understood to a decent amount as to how to deal with annotations and raw images and conversion of different file types. The other task that the two of us will be working on is going over the github repository of the winners of the VerSe competition and trying to run their model if that seems feasible. (https://github.com/christianpayer/MedicalDataAugmentationTool-VerSe/tree/master/verse2020 & https://github.com/christianpayer/MedicalDataAugmentationTool)
** Nikhil, Prahlad, and Tian are working on meta data extraction and conversion now.
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|February 21st, 2022
|February 28th, 2022
|February 23th, 2022
|-
|Review lecture 
|Completed
|February 21st, 2022
|February 28th, 2022
|February 23th, 2022
|-
|Start working on setting up models on PACE-ICE
|Completed
|February 21st, 2022
|February 28th, 2022
|February 27th, 2022
|-
|Explore Azure
|Completed
|February 21st, 2022
|February 28th, 2022
|February 24th, 2022
|-
|Sub-team meeting
|Completed
|February 21st, 2022
|February 28th, 2022
|February 27th, 2022
|-
|}
== Week 6: February 14th - February 21st(2022) == 
===Outline of class notes===
'''In-class Notes:'''
* Peer reviews open next week.
* Went through the SCRUMS of each sub-team
* Need to split tasks between each of us as we can start working on Azure now and believe that the papers that Harris (https://paperswithcode.com/paper/vertebrae-segmentation-identification-and) and I chose to read last week and summarized are a good place to start. Initially, we will also probably run experiments on PACE-ICE or Google Collab and use Azure simultaneously so we don't get to stuck up in case we face some issues using Azure.
* For this week, my task included setting up the code on the github that we got from Harris's paper along with Harris.
'''Individual Notes:'''
* We tried to set up the algorithms and code on the gitlab from the paper mentioned above (https://gitlab.inria.fr/spine/vertebrae_segmentation). However, we also faced some blockage issues on PACE and the gitlab got restricted as well, which prevented us from making too much progress on the same.
* I, then, instead decided to explore the Azure Machine Learning Studio as we have to get familiar with that platform as well.
* I created a 2 core compute node and made a Test python notebook to try and install deap and run a "Hello World" script as well after importing numpy panda and matpoilab. It worked successfully. I also explored various other ML studio features of Azure and one that I found interesting was how you could install an extension to use VScode as well, which I then got setup.
* I also explored the https://github.com/yijingru/Vertebra-Landmark-Detection repository and forked it to start working on it with Harris. We are now going to try set up this repo on PACE. One thing that I found interesting in this repo was the how they simply used better arrangement and image segmentation along with a pre-existing algorithm to get their results. I personally am also going to try and run this on Azure to get more familiar with the code and the platform.
'''Sub-team meeting Notes:'''
* We went over want everyone explored on Azure.
* We then discussed the tasking for next week. Harris and I are going to get the model for the yijingru github set up on PACE. The other three members along with Aaron are going to work try and get the DICOM images converted to jpg and label them. Aaron found a pydicom library that helped us read the images which is what we will use to understand and view the xrays now. 
* For pydicom we need to install these two packages to read the dicom files: GDCM and pylibjpeg. (https://stackoverflow.com/questions/48185544/read-and-open-dicom-images-using-python and https://pydicom.github.io/)
* I am also going to work on how to set up datasets and label them on Azure.
'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|February 14th, 2022
|February 21st, 2022
|February 17th, 2022
|-
|Review lecture 
|Completed
|February 14th, 2022
|February 21st, 2022
|February 17th, 2022
|-
|Start working on setting up models on PACE-ICE
|Completed
|February 14th, 2021
|February 21st, 2021
|February 20th, 2021
|-
|Explore Azure and get nodes and scripts set up
|Completed
|February 14th, 2022
|February 21st, 2022
|February 20th, 2022
|-
|Sub-team meeting
|Completed
|February 14th, 2022
|February 21st, 2022
|February 20th, 2022
|-
|}
== Week 5: February 7th - February 14th(2022) == 
===Outline of class notes===
'''In-class Notes:'''
* Went through the SCRUMS of each sub-team
* For this week, my task is to read: https://paperswithcode.com/paper/vertebra-focused-landmark-detection-for
* We will hopefully get access to Azure Databricks today so that we can start training ML models and running experiments on provided datasets.
'''Hospital and Sub-team meeting notes:'''
* Discussed the papers that everyone read.
* Got Azure set up.
* Not too much to note down as we were trying to figure out how Azure works and how we can use it to our advantage. 
'''Individual Notes:'''
* Got Azure set up.
* Reading through the paper for the week, "VERTEBRA-FOCUSED LANDMARK DETECTION FOR SCOLIOSIS ASSESSMENT":
** This paper has proposed a novel vertebra-focused landmark detection method. After localizing the vertebrae the four corner landmarks of the vertebra are traced through the learned corner offset
** Mentions the disadvantages of just using regression based methods (involving use of Supported Vector Regression and BoostNet) and realizes the need for using convolutional layers (U-Net) to keep order of landmarks.
** In the paper they have mentioned how each image they had in their dataset had about 17 vertebrae and hence they required about 68 landmark points proper experiments and localization of vertebrae. The method they used can be used in our experiments for localizing vertebrae and image segmentation. We can apply to our project how they marked the center of the vertebrae to localize each of them by constructing heat-maps using an unnormalized 2D Gaussian disk and a heat-map feature formula and make use of ResNet34 conv1-5 for improving the semantics of the image. 
** After going through their experimental methods, it is interesting to notice that with their significant change in vertebrae detection has lead to a major improvement in results even after using a pre- written algorithm provided by the AASCE.
** The algorithm they have mentioned is used in their GitHub: https://github.com/yijingru/Vertebra-Landmark-Detection
** They are using ground-truth landmark locations for comparing their experiments. Their method used symmetric mean absolute percentage error (SMAPE) for accuracy of measured Cobb angles.
** For the results, the paper compares regression and segmentation methods to method used in the paper. There are more results showing the significance of their method; however, our major aim is to focus on how they implemented their method and try to use that for our project.
** [[files/Conclusion.png | height=100px]] 
'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|February 7th, 2022
|February 14th, 2022
|February 10th, 2022
|-
|Review lecture 
|Completed
|February 7th, 2022
|February 14th, 2022
|February 10th, 2022
|-
|Read Research Paper
|Completed
|February 7th, 2021
|February 14th, 2021
|February 12th, 2021
|-
|Work through the bullet points mentioned in the individual notes and get familiar with labelling
|Completed
|February 7th, 2022
|February 14th, 2022
|February 12th, 2022
|-
|Get Azure setup
|Completed
|February 7th, 2022
|February 14th, 2022
|February 10th, 2022
|-
|Sub-team meeting
|Completed
|February 7th, 2022
|February 14th, 2022
|February 13th, 2022
|-
|}
== Week 4: January 31st - February 7th (2022) == 
===Outline of class notes===
'''In-class Notes:'''
* Went through the SCRUMS of each sub-team
* Discussion on finding out which method of image segmentation would be apt.
* Two image labelling tools we found: 
** https://github.com/wkentaro/labelme
** https://github.com/microsoft/VoTT
'''Hospital Meeting Notes:'''
* Went over how to use azure (live tutorial).
* Discussed timelines for when we could attain the chest x-ray datasets and azure access.
* Discussed how we could label the images.
'''Sub-team meeting notes:'''
* We decided on probably going with VoTT for image labelling.
* Looked for more research papers that we could possibly go over to decide on how we are going to implement a solution for this problem.
* We thought of how we could use CT Scans to get some idea on image segmentation, labelling and going about this for this project.
* Finally, we found a bunch of papers and split tasks amongst each one of the members.
'''Individual Notes:'''
* For this week, I had to read this paper: https://link.springer.com/content/pdf/10.1007/s00586-019-05944-z.pdf
** I went over most of the paper and found the methods used and results quite interesting.
** I don't have too much to summarize about this paper as of yet as we decided not to really implement this paper and I overviewed the paper for my own knowledge and discussed it in a sub-team meeting.
'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|January 31st, 2022
|February 7th, 2022
|February 4th, 2022
|-
|Review lecture 
|Completed
|January 31st, 2022
|February 7th, 2022
|February 4th, 2022
|-
|Read Research Papers
|Completed
|January 31st, 2022
|February 7th, 2022
|February 7th, 2022
|-
|Work through the bullet points mentioned in the individual notes and get familiar with labelling
|In progress
|January 31st, 2022
|Next week 
|Next week
|-
|Sub-team meeting
|Completed
|January 31st, 2022
|February 7th, 2022
|February 6th, 2022
|-
|}
== Week 3: January 24th - January 31st (2022)==
===Outline of class notes===
'''In-class Notes:'''
* Went through the SCRUMS of each sub-team
* The meeting today will be ended earlier so that our team can have a meeting with the representatives of Schriner's Hospital to attain data for our experiments and ask questions regarding how we can go about the tasks and objectives for our sub-team.
'''Hospital Meeting Notes #1:'''
* Schriner's Children's three part mission has the goal to change and improve lives
* Idiopathic Scoliosis:
** Incidence is estimated to be 1-4 per 1000.
** More prevalent in females.
** Of unknown cause
** Treatments include observation, non surgical intervention and surgery.
* Use DICOM image analysis
* Scalability -- current limitations include:
** Human expertise in x-ray analysis
** Isolation of data
** Limited big data
** Network and security concerns
*<b> Potential solution: Federated ML at the "Edge"</b>
* Try and reach more children using these designs and algorithms.
* Software application being used: Azure -- access to ML tools
** Azure Databricks (have to see how we can work on editing python notebooks and integrate scripts)
* Top 5 most useful angles out of around 22 that we can work on:
** Cobb
** Apex of Thoracic Curve
** Apex of Lumber Curve
** Looking at how centered is the patient's head over their hip
** Tilt of the shoulder measured off the top of the T1 vertebrae
'''Hospital Meeting Notes #2:'''
* Discussed how to calculate the different angles mentioned above.
* Talked about how to work around the complications that we could possibly face while measuring these angles.
* Got Box set up to share documentation
* Dealt more with anatomy
'''Sub-team meeting:'''
* Discussed how we could start selecting primitives and work with data pre-processing using few similar images that we could find on the Internet (haven't found any such dataset similar to what we are expecting from the hospital as of yet).
* We are thinking of working with U-Net convolutional neural network for data analysis and labelling. 
* We are thinking of using OpenGL primitives for angle measurement.
'''Individual Notes:'''
* I have been reading more of the given research papers; however, I am still trying to understand a few different concepts that I am going through.
* I got Box set up after setting up another appointment with the representative at Schriners Hospital.
* Currently, working on going over U-Net research and other research papers that will help in image segmentation.
* Thinking of an overall flow using multiple research papers.
* Figuring out which OpenCV primitives may be a good fit for this problem.
* We will require labelled data for image segmentation.
* Flow of the work that we could start doing soon: 
** Label Data -> Image Segmentation
** Neural Network only (Will require overlap with NAS team)
** Find the seed architectures through literatures. (and seed EMADE with this)
** NN for image segmentation to find vertebrae, label center of sacrum.
** Can we do image enhancement at a minimum to feed image segmentation to find/using certain techniques?
* For this week, I haven't really summarized any of the findings in a research paper as I only skimmed through about two of the papers. My main tasks for this week were to set up Box and come up with any questions or thoughts regarding image processing. Few things that I had thought of were:
** Could we start with unsupervised learning for labelling the images provided in the dataset?
** For image labelling, I thought that using Vott could be a good idea, which Aaron also suggested. I thought so as if we are using Azure for most of our computations, maybe using another Microsoft developed repository for image labelling could be helpful.
** This was one of the research papers that I went over this week:
https://www.hindawi.com/journals/cmmm/2019/6357171/
Just a small summary of few things I noticed is mentioned below.
*** We could possibly go over x-rays, CT Scans and MRIs in order to get more familiar with spinal frontal view images and work on how to start image segmentation.
*** CNNs are definitely more of a suitable choice rather than using traditional machine learning methods as they do not require any handcrafted features for training and can be trained for object detection and semantic segmentation.
*** U-Net models have been used in this paper and they have developed an automatic way to measure the curvature of the spine.
*** We could make use of how they have proposed isolating the spine region of interest (ROI) using column separation and focusing on mean intensity. Vertebrae detection in this paper has been done in multiple steps, firstly starting off with finding the Central Line Segment, then going of detecting the boundary using pixel maximum intensity recognition and finally using a computation formula then to detect the vertebrae.
*** For vertebrae segmentation, they applied three different CNNs: U-Net, Residual U-Net, and Dense U-Net. Based on this, after obtaining the the convolutional layers and performing rectified linear unit (ReLU) and batch normalization (BN), the formula used for convulation was: 
h(xl) = W^T * xl + b
where  xl and h(xl) are input and output in the lth layer of the convolution, respectively, W^T is a learning filter of the convolution, and b is a bias.
*** The rest of the paper goes over dense U-Net architecture and Cobb angle measurement which I went over in brief and have read a bit about in the paper I read last week. I am going over how these architectures and measurements work as well.
*** The results talked about comparing the final conclusions with the manual results of doctors. Some of the comparisons and values mentioned the table given in the research paper seemed quiet fascinating; however, the researchers are still going over improved ways of the Cobb angle measurement and how to better implement the three CNNs.
**This was the other paper I skimmed over but couldn't take note of too much-- http://spineweb.digitalimaginggroup.ca/Index.php?n=Main.Datasets#Dataset_16.3A_609_spinal_anterior-posterior_x-ray_images I will try and do more next week.

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|January 24th, 2022
|January 31st, 2022
|January 27th, 2022
|-
|Review lecture 
|Completed
|January 24th, 2022
|January 31st, 2022
|January 27th, 2022
|-
|Read Research Papers
|In progress
|January 24th, 2022
|January 31st, 2022
|January 27th, 2022
|-
|Get BOX set up
|Completed
|January 24th, 2022
|January 31st, 2022
|January 27th, 2022
|-
|Work through the bullet points mentioned in the individual notes
|In progress
|January 30th, 2022
|Next week 
|Next week
|-
|Sub-team meeting
|Completed
|January 24th, 2022
|January 31st, 2022
|January 30th, 2022
|-
|}

== Week 1 & 2: January 10th -  January 24th (2022)== 
===Outline of class notes===
'''In-class Notes:'''
* Meeting was virtual this time.
* Discussed more about what the sub-teams had done in the previous semester and what they are looking forward to do.
* I plan on working in the Image Processing sub-team as I really found the work we did during the last few weeks of Fall 2021 quite interesting. We just need to vote for a new leader as Maxim Daniel Geller has now left the VIP.
* New teams were then proposed.
** Application Development team to simplify usage of EMADE (prevent too much of work using the command line)
** Infrastructure
** Scoliosis measurements proposed by Dr. Greg Rohling.
* I really liked the idea of this sub-team proposed by Dr. Rohling and have considered joining this team as of now as it is very similar to image processing but more bent towards my interests.
* The main description of this sub-team is:
** Scoliosis is a three-dimensional deformity that requires careful planning of treatment to successfully correct the deformity in all three planes. Measurement of the Cobb angle is a widely accepted method used to gauge the severity a patient's coronal deformity, and plays a large part in determining whether a patient requires surgical intervention. The purpose of this study is to determine if artificial intelligence approaches can be used to measure Cobb angles and provide care recommendation classifications for patients with idiopathic scoliosis.
** We will be working on solving this problem and our work will be guided by the following research paper. 
https://drive.google.com/file/d/1h5mu4POaTh_m3gJgP4Z3aWPjnZPB-p1U/view?usp=sharing
* I look forward to working on this project and understanding more about the contents with Dr. Rohling on 1/24/2022.

'''Sub-team meeting on 1/16:'''
* Discussed questions and important findings mentioned in the research paper.
* We set up a google drive for all relevant meeting notes, requirements, and tasks.
* Decided that we should use PACE for running ML models and looking for a solution to this problem.
* We are currently working on noting down questions or doubts that we have regarding the logistics or kind of data that we are going to receive once we meet with the representatives from the hospital.
* We are also working on taking inspiration from the image processing team to pre-process the data that we get later on. 
* Not much work to do as of now as I have already completed setting up PACE and am just doing my own research right now based on scoliosis in AIS.

'''Sub-team meeting on 1/23:'''
* Talked with Dr. Rohling to figure out the objectives of our sub-team.
* Helped members set up on PACE-ICE.
* We have a meeting scheduled with the hospital on 1/24
* The current objectives for our team are:
** Perform a literature search on x-rays, scoliosis, and ML methodologies.
** Go through the azure platform that we will be using with the hospital.
* Few of our other <b>action items or questions</b> include:
** Can this problem be EMADE-ified?
** How can we preprocess the data using Automated Algorithm Design (masking out the vertebrae) to run our experiments on the same?
** How well can we come up with an algorithm that outlines vertebrae?
** Can we find something that gives us its orientation relative to view angle?
** Which vertebra has a maximized Cobb angle?

'''Individual Notes:'''
* The research paper was quite an enriching read.
* The parts I found the most interesting were related to the use of deep learning algorithms and image processing.
* I am currently doing my own research on image processing algorithms. Being a part of the image processing team previously has helped.
* The purpose of this paper is to evaluate the performance of 3D depth sensor imaging system to predict COB angle in AIS using deep learning algorithms.
* We want to be able to detect AIS on time.
* The method utilized in this paper involves 160 data files being shuffled into five datasets.
* Each of these datasets consists of 32 files at random after which a five-fold cross-validation was performed.
* The predicted performance was evaluated using Mean of Absolute value of Errors and Root of the Mean of the Square of Errors between the actual and predicted COB angles.
* I am still slightly confused with the way the CNN models are depicted and explained (I have gained a broad understanding but want to know more). Also, I am doing my own research to comprehend the final results in a more effective manner.
'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|January 10th, 2022
|January 24th, 2022
|January 22nd, 2022
|-
|Review lecture 
|Completed
|January 10th, 2022
|January 17th, 2022
|January 12th, 2022
|-
|Read Research Paper
|Completed
|January 10th, 2022
|January 17th, 2022
|January 15th, 2022
|-
|Sub-team meeting 
|Completed
|January 10th, 2022
|January 17th, 2022
|January 16th, 2022
|-
|Sub-team meeting on 1/23
|Completed
|January 17th, 2022
|January 24th, 2022
|January 23th, 2022
|-
|}

= Fall 2021 =
== Week 16: December 6th - December 12th(2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Checked if any teams had any issue before their final presentations.

'''Lecture Notes:'''
* We assigned tasks to each individual for the final presentation.
* I will be working on learning and understanding more about hyper-features and slides for mating and mutation methods.

'''Sub-Team Meetings, Individual Contributions, and Final Presentation preparation notes:'''
* I worked with Aryaan to get the mating and mutation methods (mainly mating changes) xml file to get configured accordingly. 
* As we still had to run trials for depicting the results for our final presentation, so till the time mating xml file was ready I started running trials with Maxim for the baseline trial runs.
* xml used for baseline runs: https://drive.google.com/file/d/1ii0FLt3TmwInMJIyEC5UveRO3wMyLxkQ/view?usp=sharing
* I completed two baseline trial runs (by running EMADE with a specifically configured xml file on PACE) and sent over my dump folder with all information in my mysql database for the results of the final presentation.
* After the final xml file for mating and mutation runs was ready, I started a trial run for the same.
* xml file that I used for the mating methods runs: https://drive.google.com/file/d/1F2xUBj-HeYVGaWXaIjvnReWtjr9e8KhQ/view?usp=sharing
* After about an hour or so of starting the run, the EMADE_mastero##### crashed and lost its connection to the mysql server, giving me an error and also giving just an empty set when I tried to find the best individuals (not duplicated) in this run.
* To fix this and prevent the master from crashing, I changed the reuse value to 0 from 1, killed all my previous mysql jobs and started a new job.
* After doing this I was able to successfully run a trial for the mating and mutations methods and give out an output that we used in our final presentation.
* While preparing for the final presentation, I covered how the number of parameters effect the AUC ROC results and that the denser the trees (more parameters) are the better the area under the curve became.
* I also made two of the slides for the same and depicted the results by using trees and tables.
* We figured that it was the AUCs for the Pareto graphs was important to focus on, which is why we went over AUC ROC first. If we ran more trials we could have possibly gotten better results.
* To generate trees, we used help from the following link: https://github.com/cwhaley112/TreeVis
* I then used the command query, "mysql -u rbatra34 -p -D mating_runs -e "select tree,\`FullDataSet ROC AUC\`, \`FullDataSet Num Parameters\` from individuals ind join paretofront pf on pf.hash=ind.hash where pf.generation = (select max(generation) from paretofront);" | tr '\t' '|' > ./mutations.csv", to generate a csv file that I used for different generations and runs to create a tree representation.
* This is the link to the csv file that I generated (for mating methods): https://drive.google.com/file/d/1kS-jaAth1kURyc79u7-KXTYea_fmbkJw/view?usp=sharing
* This is the link to the csv file that I generated (for baselines runs): https://drive.google.com/file/d/1xGdtggXg0K-VdidE1KnaycHKruG2HaU7/view?usp=sharing
* I used the following command query to generate a table representing the AUC ROC: "select `FullDataSet ROC AUC`, `FullDataSet Num Parameters` from individuals ind join paretofront pf on pf.hash=ind.hash where pf.generation = (select max(generation) from paretofront);"
* This generated the following table (mating):
[[files/TableData.png|height=100px]]

* '''Bonus:''' 
** Some research sources: 
*** Understanding more about ROC: https://stackoverflow.com/questions/44172162/f1-score-vs-roc-auc
*** Understanding more about the mating and mutations methods: Went over the previous meeting notes over the last semester and this semester (during the time when I was in bootcamp)

'''Final-Presentation Day:'''
* All my results and slides had been added onto the slides.
* I was ready to prepare my thoughts and results in front of the class.
* During the presentation, I talked about the slides that I wrote (as mentioned above in the preparation notes).
* Our presentation went well.
* After getting reviews from the Dr. Zutty and Dr. Rohling, we decided that we should now try focusing on specific features to improve our results and algorithms. For example, we can focus on perfecting lexicase selection methods in order to provide a large number of accurate results and trials.
* We can definitely improve our output over the course of next semester.
* I then listened to other sub-teams presentations and really enjoyed the work that the Neural Architecture Search team has been doing.
** '''Natural Language Processing (NLP):'''
*** I really liked seeing how they are working on focussing on simpler classifications and type selections first.
*** Improving and enhancing a pre-existing retained model is vital. 
*** Used AutoML to upgrade their Q/A models.
*** The NNLearner2 has an ARG0 and ARG1 as they have a number of outputs.
*** They are use expressive over abstractive to be able to first try and solve simpler data.

** '''Stocks:'''
*** I liked their use of analyzing the primitives and profits.
*** Used AutoML with EMADE.
*** The are using ARGmin and ARGmax and aren't working with ML models as much, which is quite interesting and out-of-the-box.

** '''Neural Architecture Search:'''
*** I found it quite interesting mainly I was intrigued to see the vast variety of results they obtained over the course of the semester. 
*** Adding mutation and mate methods for better accuracies.
*** Worked on the automated hyper-parameter optimization.
*** I was also impressed with their visualizations and representations and how multiple groups uses their work to develop on their own projects.

**'''Modularity:'''
*** Able to understand more about building and using ARLs and figured that the nested ARLs are pretty advanced to work with.

Link to Final Presentation: https://docs.google.com/presentation/d/1c6c51KhAIRJMMRo2mzNDDFsWm_MkFgXmNa3j7aLppdk/edit?usp=sharing

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|December 6th, 2021
|December 12th, 2021
|December 10th, 2021
|-
|Review lecture notes
|Completed
|December 6th, 2021
|December 12th, 2021
|December 10th, 2021
|-
|Run trial runs
|Completed
|December 6th, 2021
|December 12th, 2021
|December 10th, 2021
|-
|Prepare Final Presentation
|Completed
|December 6th, 2021
|December 12th, 2021
|December 10th, 2021
|-
|Individual Research on mating xml configurations and tree visualizations
|Completed 
|December 6th, 2021
|December 12th, 2021
|December 10th, 2021
|}

== Week 15: November 29th - December 6th(2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Went through the SCRUM for each group

'''Lecture Notes:'''
* We discussed the objectives for our sub-team for the rest of the semester.
* Not too much progress was made in the previous week as it was Thanksgiving break.
* We aim to start more experiments to be run on EMADE.
* Finally, our aim is to prepare the final presentation soon.

'''Sub-team Meeting:'''
* Assigned tasks for the following week.
* We have to start getting our presentation ready.
* I will be working with Monil and Heidi to prepare the hyperfeatures slides.
* '''Update''': I was initially doing my research on hyperfeatures by going through all past meeting notes of the vip, but as hyperfeatures was already managed by Monil and Heidi, I joined Aryaan to help with mating and mutation methods and run trials before our final presentation.
* Rest of the meeting was for working and coding. I attempted to get EMADE running on PACE.

'''Individual Notes:'''
* Finally was able to solve the MySQL server connection issues on PACE.
* I was working in the right direction by using lsof -i:portnumber to delete any preexisting instances of MySQL but turns out I only deleted the instance of my current port. I had used another port (3302) earlier and with Max's advice I was able to figure out that this was the issue and hence, solved the MySQL server issues.
* After MySQL started running, my objective was to get EMADE running.
* I faced various issues while running the launchEMADE.pbs script.
** Hanging tag in the xml file.
*** Fixed this by going over the xml file to find the incorrect syntax.
[[files/FirstError.png|height= 300px]]
** Outdated inputSchema file.
*** To fix this I updated this file by switching over to the latest Image-Processing(nn-vip) branch of the forked emade repository.
[[files/SecondError.png|height= 300px]]
** Incorrect location of the chestxray datasets.
*** Corrected this by altering the path location using /storage/home/hpaceice1/shared-classes/materials/vip/AAD/ip_group.
[[files/ThirdError.png|height= 450px]]
** Incorrect mysql server node address in xml file.
*** Fixed this by updating the node I was ssh-ed into.
* All of these changes finally got EMADE to run on PACE successfully without any errors.
[[files/FourthCompletion.png|height= 300px]]

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|November 29th, 2021
|December 6th, 2021
|December 1st, 2021
|-
|Review lecture notes
|Completed
|November 29th, 2021
|December 6th, 2021
|November 30th, 2021
|-
|Get EMADE to successfully run on PACE
|Completed
|November 29th, 2021
|December 6th, 2021
|December 5th, 2021
|}


== Week 14: November 22nd - November 29th(2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Went through the SCRUM for each group

'''Lecture Notes:'''
* We discussed how our data is almost ready and everyone from the bootcamp is almost setup with PACE now. We also talked about what we did over the week, such as running new mutation methods and finalising hyperfeatures.
* We then went over the work of every other group, discussed multiple bugs that everyone is facing, and brainstormed on finding the apt solutions to debug any problem.
* We worked on fixing multiple PACE issues (mainly with MySQL).

'''Individual Notes:'''
* Fixed the issue of 'Device or resource is busy' by using the command 'lsof +D' and the kill() command to stop all open mysql files from running. This allowed me to reinstall mysql that I needed to do to setup PACE.
* Currently, I am facing "Can't connect to server" issues.
* Working on adding the pbs scripts and fixing the MySQL problems.
* I am mentioning all the errors and issues I am facing in getting PACE setup to ensure I do not repeat them in the future and can work through them successfully.

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|November 22nd, 2021
|November 29th, 2021
|November 22nd, 2021
|-
|Review lecture notes
|Completed
|November 22nd, 2021
|November 29th, 2021
|November 22nd, 2021
|-
|Set up PACE
|In progress
|November 22nd, 2021
|November 29th, 2021
|November 29th, 2021
|-
|Individual Research 
|Completed 
|November 22nd, 2021
|November 29th, 2021
|November 25th, 2021
|}

== Week 13: November 15th - November 22nd(2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Went through the SCRUM for each group

'''Lecture Notes:'''
* We discussed how each of the groups is working towards developing the final presentations for the end of the semesters and obtaining the desired objectives.
* Worked on multiple objectives such as data pre-processing and sharpening features for the hyperfeatures team.
* Bootcamp students worked on setting up PACE. It is almost done but many issues are being encountered, which are being solved as we move forward.

'''Sub-team meeting Notes:'''
* Workday
* Helped each other with multiple processes like processing data, look for the apt research papers for selection methods, mating and mutation methods etc.
* We also tried resolving other PACE issues. 

'''Individual Notes:'''
* Tried fixing the disk storage issue by removing all previous conda environments. It didn't fix the issue entirely.
* I asked Dr. Zutty why I am facing 'Disk Quota Exceeded' issues. With his help and after using scripts like, "du -h ../" and "ls -larth" to find which dataset is taking uo most of the memory space. 
* Turns out there was one extremely large dataset that hadn't been removed because of which I was facing conda environment issues as PACE only offers a certain fixed amount of memory to each individual.
* Currently facing some issues while installing tensorflow in the conda environment.
* All other packages have been installed.
* I read few papers and did research on how sharpening hyperfeature can be improved.
* I worked on setting up PACE. 
* Faced multiple issues towards the end and now I am deciding to reinstall my MySQL and then run the steps again.
* The current error that I am getting is, "rm: cannot remove ‘db/.nfs000000007ed4a9f2000006d5’: Device or resource busy", when I try and remove all MySQL files.
* I am trying to fix this error by using the internet and taking help from sub-team members on Slack. 

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|November 15th, 2021
|November 22nd, 2021
|November 16th, 2021
|-
|Review lecture notes
|Completed
|November 15th, 2021
|November 22nd, 2021
|November 15th, 2021
|-
|Sub-team meeting
|Completed
|November 15th, 2021
|November 22nd, 2021
|November 17th, 2021
|-
|Set up PACE
|In progress
|November 15th, 2021
|November 22nd, 2021
|November 22nd, 2021
|-
|Individual Research 
|In progress
|November 15th, 2021
|November 22nd, 2021
|November 22nd, 2021
|}

== Week 12: November 8th - November 15th(2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Went through the SCRUM for each group

'''Lecture Notes:'''
* Our team mentioned our objectives for the rest of the semester. We decided to create some new developments for improving hyperfeatures such as creating primitives for checking/improving the sharpening characteristics of the images. Also, we are trying to develop multi-class algorithms to classify images as per the diseases and check whether a disease is present.

'''Sub-team meeting notes:'''
* Workday 
* Tried understanding more about hyperfeatures and seeting up PACE.
* Faced the following issue: ERROR 2002 (HY000): Can’t connect to local MySQL server through socket ‘scratch/db/mysqldb.sock’ (2)

'''Individual Notes:'''
* Solved this issue by using SQL directly through the login node and then ssh into the compute node.
* Solved login issues by making sure I was logged into SQL through the root user.
* Faced issue: [Errno 122] Disk quota exceeded
* Currently working on solving this issue

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|November 8th, 2021
|November 15th, 2021
|November 10th, 2021
|-
|Review lecture notes
|Completed
|November 8th, 2021
|November 15th, 2021
|November 8th, 2021
|-
|Sub-team meeting
|Completed
|November 8th, 2021
|November 15th, 2021
|November 10th, 2021
|-
|Set up PACE
|In progress
|November 8th, 2021
|November 15th, 2021
|November 15th, 2021
|}
== Week 11: November 1st - November 7th(2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Got assigned to sub-teams
* Went through the SCRUM for each group

'''Lecture Notes:'''
* I was assigned to the image processing sub team.
* Got familiar with what the team is working with and introduced myself to the team members.
* Told to clone the forked EMADE repository, read a research paper, and try to get PACE to work on my individual system

'''Sub-team meeting Notes:'''
* Had a round table discussion about what each group (out of Selection method group, data preparation group, mating and mutation method group, and hyperfeatures group) in the sub team has accomplished in this week.
** Selection: Lexicase vs NSGA II
** Hyper Features: Going through papers to understand implementing primitives.
** Mating and mutation: Creating new methods
** Data Preparation/infrastructure: Fixing bugs and prepping EMADE.
* Discussed possible solutions to every issue discussed. One issue faced is regarding the fact the trees are difficult to be converted to binary, which is leading to problems in creating datasets.
* The main branch in the sub-team's EMADE is not master and is cachev2.
* We are using nn-vip branch(Neural Architecture Search) through which we use the main Image-Processing(nn-vip).
* No deep learning in this main cachev2 branch.
* Path to our shared directory on PACE to access all datasets and be a part of the image processing group: root/storage/home/hpaceice1/shared-classes/materials/vip/AAD/ip_group
* You may need to seed your data with NN learners before running a fresh EMADE run on PACE. -- seeding_from_file.py xml.
* Assigned to the hyperfeatures subgroup.

'''Individual Notes:'''
* Cloned the forked EMADE repository.
* Read a part of the "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning" (https://arxiv.org/pdf/1711.05225v3.pdf) and the "Concurrent Neural Tree and Data Preprocessing AutoML for Image Classification" papers.
* I read through how each of small features in multiple images makes a major difference in classification later on and did my own research to understand the difference between multi-class and multi-label.
[[files/ChexnetImage2.png|height= 500px]] [[files/ChexnetPaperImage1.png|height=500px]]
* Did some research about PACE. Couldn't work too much with PACE as it was under maintenance. 
* Tried to configure PACE. Faced some issues with SCP transferring
* I fixed the SCP transferring issue by changing the default port number to my own custom port number in the cnf file. 

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|November 1st, 2021
|November 8th, 2021
|November 3rd, 2021
|-
|Review lecture notes
|Completed
|November 1st, 2021
|November 8th, 2021
|November 1st, 2021
|-
|Sub-team meeting
|Completed
|November 1st, 2021
|November 8th, 2021
|November 3rd, 2021
|-
|Cloned forked EMADE
|Completed
|November 1st, 2021
|November 8th, 2021
|November 1st, 2021
|-
|Read papers and worked with PACE
|In progress
|November 1st, 2021
|November 8th, 2021
|November 7th, 2021
|}

== Week 10: October 25th - November 1st(2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Final Presentation Day

'''Lecture Notes:'''
* Understood more about the subteams like NLP, NAS, Image processing etc.
* Presented our work and findings on ML, MOGP, and EMADE models.
* Answered questions relating to why we were only able to run 10 generations.
* Mentioned our mistakes and how we could improve them.
* Talked about the multiple graphs we used in our presentation.

'''Individual notes:
* Talked about MOGP results and EMADE installation/difficulties.
* Mentioned how we could have improved our findings and answered questions regarding the graphs and generation issues.

'''Presentation Link:'''https://docs.google.com/presentation/d/1ShDz-7hPoor3ExWA9BKqiSzqn-G4ufgBYWor-mtlzdU/edit?usp=sharing

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|October 25th, 2021
|November 1st, 2021
|October 25th, 2021
|-
|Review lecture notes
|Completed
|October 25th, 2021
|November 1st, 2021
|October 25th, 2021
|-
|Presented our findings
|Completed
|October 25th, 2021
|November 1st, 2021
|October 25th, 2021
|}

== Week 9: October 20th - October 25th (2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Workday with VIP almuni

'''Lecture Notes:'''
* Configured how to run all processes (master and workers)
* Joined the master server

'''Sub-Team meeting Notes:'''
* Worked out how to join worker processes and speed up the generation process.
* Plotted graphs for the different results we had.
* Prepared the presentation slides and accumulated the results for ML, MOGP, and EMADE.

'''Individual Notes:'''
* Gave the idea for using a virtual conda environment to fix the invalid python version issue.
* Helped others run EMADE and join the worker processes.
* Wrote the code for plotting graphs along with two other team members.
* Worked on the presentation slides and did more research on EMADE.

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|October 20th, 2021
|October 25th, 2021
|October 24th, 2021
|-
|Review lecture notes
|Completed
|October 20th, 2021
|October 25th, 2021
|October 20th, 2021
|-
|Worked on multiple issues (Worker Process)
|Completed
|October 20th, 2021
|October 25th, 2021
|October 24th, 2021
|-
|Created virtual conda environment
|Completed
|October 20th, 2021
|October 25th, 2021
|October 23th, 2021
|-
|Sub-Team Meeting
|Completed
|October 20th, 2021
|October 25th, 2021
|October 24th, 2021
|}


== Week 8: October 13th - October 20th (2021) == 
===Outline of class notes===
'''In class:'''
* Work on trying to get EMADE to run.

'''Lecture Notes:'''
* Workday.
* Join a common MySQL server made by one team member. Use the following command to do so: "mysql -h hostname -u username -d database_name -p"
* Dr. Zutty answered questions and helped resolve difficulties relating to running EMADE.

'''Sub-Team meeting notes:'''
* We were able to successfully get EMADE to recognize the MySQL database we were using.
* Worked on further methods and steps to start with the generation process.
* Faced a major issue such that the fitness values for individuals were always infinity.
* As we were facing issues with the data types for our primitives as well, we believe that the issue may be related to the false positive and false negative evaluation functions that are defined in evalFunctions.py.

'''Individual Notes:'''
* Did a lot of research on how to fix the (inf, inf, inf) issue. Tried asking research alumnis for any advice as well.
* Helped develop the evaluation functions with the team.
* Worked on the process of joining the MySQL server for worker processes. Faced some issues with python version
* Attempting to fix this python version issue by creating a virtual conda environment. Got this idea after doing some research.

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|October 13th, 2021
|October 20th, 2021
|October 17th, 2021
|-
|Review lecture notes
|Completed
|October 13th, 2021
|October 20th, 2021
|October 14th, 2021
|-
|Worked on multiple issues
|Completed
|October 13th, 2021
|October 20th, 2021
|October 20th, 2021
|-
|Tried creating virtual conda environment
|In progress
|October 13th, 2021
|October 20th, 2021
|October 20th, 2021
|-
|Sub-Team Meetings
|Completed
|October 13th, 2021
|October 20th, 2021
|October 16th - 19th, 2021
|}

== Week 7: October 6th - October 13th (2021) == 
===Outline of class notes===
'''In class:'''
* Reminded of notebook completion for mid term grading.
* Introduction to EMADE and MySQL.
* Final Presentation is on October 25th, 2021.

'''Lecture Notes:'''
* Introduced to EMADE (Evolutionary Multi-Objective Algorithm Design Engine) and understood the conceptual working of this engine.
* It combines/integrates the Multiple objective evolutionary search with advanced primitives to automate the designing of Machine Learning algorithms.
* Understood more about the input .xml file that is required for fulfilling our objectives of minimization.
* Discussed importance of evolution of parameters.
* Comprehended and went over the EMADE structure, headless chicken crossover, and final output of EMADE.
* Assigned tasks for the week:
** Install EMADE, using the instructions given in the README.md file
** After installation of all requirements, we have to run EMADE on the Titanic dataset as a group.
** Important to connect worker process to peer using "python src/GPFramework/launchGTMOEP.py templates/input_titanic.xml -w".
** Understand the usage of SQL and make sure to plot graphs and diagrams for final presentation on October 25th, 2021.


'''Sub-Team meeting Notes:'''
* All team members were able to successfully install EMADE and MySQL on their laptops
* Configured the input XML file for EMADE so that we can run the Titanic dataset.
* Worked on getting familiar with MySQL and run EMADE on titanic dataset.
* One-hot encoded "Embarked" feature in dataset to improve the algorithm and reflect changes under our MOGP assignment as well.
* Dealt with some pythonic issues, which we planned on fixing in the next meeting. Issue was related to evaluating individuals in queue.


'''Individual Notes:'''
* Completed all required mentioned in the README document before the sub-team meeting.
* Already had git and MySQL installed.
* Cloned the emade repository.
* Researched more on the working of MySQL and practiced using it with MySQL workbench.
* Tried to solve issues faced during sub-team meeting and debug the algorithm we were developing on the Titanic dataset. Reached out to Dr. Zutty to resolve the issue with XML.


'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|October 6th, 2021
|October 13th, 2021
|October 12th, 2021
|-
|Review lecture notes
|Completed
|October 6th, 2021
|October 13th, 2021
|October 8th, 2021
|-
|Installed all required softwares
|Completed
|October 6th, 2021
|October 13th, 2021
|October 11th, 2021
|-
|Sub-Team Meeting
|Completed
|October 6th, 2021
|October 13th, 2021
|October 12th, 2021
|}

== Week 6: September 29th - October 6th (2021) == 
===Outline of class notes===
'''In class:'''
* Presentation Day

'''Lecture Notes:'''
* Listened to presentations on MOGP and ML findings of other groups and asked questions.
* Presented our own analysis of the ML and MOGP findings. We compared these observations.
* Had a discussion about the changes and improvements we could have made in our algorithm. We should have used one-hot encoding for columns like 'Embarked'. This would have helped us convert alphabetical values into binary numerical values rather than non binary values. This could have increased our algorithms accuracy.
* Sub Team #2 Presentation: https://docs.google.com/presentation/d/1E5DIPJOt7uBeqUeYklg6TE7X7PTdOsaFdUjTDCrttkU/edit?usp=sharing

'''Individual Notes:'''
* Presented and explained the slides including Data Pre-Processing Procedure and the final comparison between MOGP and ML findings.
* Wrote the code for generating a csv file containing the best MOGP algorithm results and submitted it as a group.
* Completed Mid-Term Peer Evaluation for my teammates.

CSV File: https://drive.google.com/file/d/1U3DTjQCR9zZx5LvaO1SqspM6T-hH7Dig/view?usp=sharing

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 29th, 2021
|October 6th, 2021
|October 4th, 2021
|-
|Review lecture notes
|Completed
|September 29th, 2021
|October 6th, 2021
|September 30th, 2021
|-
|Peer Evaluations
|Completed
|October 4th, 2021
|October 6th, 2021
|October 8th, 2021
|}

== Week 5: September 22nd - September 29th (2021) == 
===Outline of class notes===

'''In class:'''
* Discussed last week's (Titanic Dataset) project.
* Got notified of this week's task.
* Learned how to give presentations.
* Had a discussion with group members as to when to schedule upcoming meetings.

'''Lecture Notes:'''
* While discussing last week's project, we found out that changing the hyper parameters was an important factor required for obtaining co-dominant algorithms.
* Sometimes, this change in hyper parameters led to a reduction in the Pareto optimal solutions/fitness scores.
* Completing the objective of obtaining co-dominant solutions required hit and trial and a few iterations for finding the apt models to achieve a successful outcome.
* For next week we have to use multiple objective genetic programming to find the Pareto optimal solutions for the same Titanic Dataset. This time we don't have any constraints such as finding a set of co-dominant algorithms. 
* Our aim is to improve the accuracy of our algorithm to find the best optimal solutions.
* We have to use basic primitives, such as logical and mathematical, to generate desired solutions.
* We are supposed to write our own algorithms for selection, crossover, mutator functions and not use tournament selection.
* Finally, we have to compare the Pareto frontier values (ML and GP) and then submit a csv file.
* Generation of own genetic loop is important and presentation of work and algorithms will take place next week.
* <b>Presentation about how to give a presentation:</b>
** Important to follow mentioned organized structure.
** Should present required schedule tables and graphs that have been plotted.
** Explain how you came up with the solution.
** Text can be on the slides but try not to read off the text directly.

'''Sub-team Meeting Notes:'''
* Discussed the steps required to attain desired output through the python notebook.
* Used primitives from Lab 2 to fulfill the requirements of this Lab.
* Experimented multiple models and algorithms to find the apt one for selection as we were not allowed to use tournament selection. We used SPEA2.
* Used symbolic regression evaluation function.
* Decided content to be added to our presentation.
* Programmed our own genetic loop.

[[files/VIPgraph.PNG|center|thumb]] [[files/GraphImage.png|height=300px]] 

'''Individual Notes:'''
* Did a lot of research on working of symbolic regression evaluation function and difference between NSGA II and SPEA2 for selection algorithms. Finalized evaluation function and data selection.
* Worked on programming the genetic loop. As it had to be completely original, coding one took a while and required each one of us to do our own research for the same.
* I designed the layout of the presentation and was responsible for verifying the code and information to be included.
* Added code for area under the curve to ensure an apt comparison of ML and MOGP.

'''Presentation:''' https://docs.google.com/presentation/d/1E5DIPJOt7uBeqUeYklg6TE7X7PTdOsaFdUjTDCrttkU/edit?usp=sharing

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 26th, 2021
|-
|Review lecture slides
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 25th, 2021
|-
|Held team meeting #1
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 26th, 2021
|- 
|Held team meeting #2
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 28th, 2021
|- 
|Completed Python notebook
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 29th, 2021
|- 
|Made Presentation
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 28th, 2021
|}

== Week 4: September 15th - September 22nd (2021) == 
===Outline of class notes===

'''In class:'''
* Sub-Teams assigned
* Discussion of Titanic dataset
* Overview of Machine learning libraries, especially Sci-kit Learn

'''Lecture Notes:'''
* Got assigned into sub-teams using our Pareto optimality (decided by the Professor)
** I have been assigned to Sub-Team #2 consisting of me, Manas, Aditya Kumaran, Adhitya Gurunathan, and Yisu Ma.
* Introduced to Kaggle, an online community based platform for data science and machine learning purposes.
* Discussed the Titanic Dataset on Kaggle.
** The Titanic Dataset: The titanic problem is used for designing a model which can accurately predict whether or not a given passenger will survive on the basis of the data. Our task is to come up with co-dominant algorithms and submit each of our predictions in the form of a csv file on Canvas.
** Codominant Algorithms: Algorithms in which one does not dominate another on every objective. For this particular challenge, we have to classify survivors and minimize false positives (FP) and false negatives (FN).
* Went over the sample code/Python notebook related to this dataset. The notebook used sci-ket learn for modeling.
* Towards the end of the lecture, we broke out into our sub-teams, noted one another's contacts, and set up a meeting outside of lecture time.

===Titanic Dataset (ML)===
'''Sub-Team meeting Notes:'''
* Created a Discord Chat to keep in touch, share ideas, and schedule meetings.
* Scheduled a virtual meeting at 2:45 pm on 18th September. 
* Set up the Python notebook for the Titanic Challenge and opened it in Jupiter using the Anaconda Navigator.
* Discussed the parameters/features that our not vital for improving the accuracy of our Machine Learning algorithm.
* Removed the following parameters, as they did not affect the passenger survival rate significantly:
** Name
** PassengerID
** Ticket Number
** Fare
* We kept the 'Embarked' and 'Sex' parameter and mapped the alphabetical values to numerical ones for easier algorithmic comprehension using a dictionary (column map). This parameter was important as the port you boarded from would affect which cabins were available. Hence, this would have an impact on where one was when the ship sank.
* The impact of the parameters on the survival rate were reasons like not willing to separate from their family, class, age, gender, etc.
* We then created a NaN_map to fill in the missing Age and Embarked values.
* We tried using different models using the Sci Kit learn documentation. We used the RandomForestClassifier and the MLP classifier initially, which were co-dominant algorithms. We changed their parameters constantly to find the best optimal solutions. We imported the 'ensemble' class to run RandomForestClassifier.
* Then went through the documentation more to find co-dominant algorithms. After trying various algorithms we finally found three others, AdaBoostClassifier, SVM, and DecisiomTreeClassifier. We changed the parameters around in order to ensure that the algorithms were co-dominant. These changes led to a change in the algorithms accuracy, but the change was not large enough to affect the objective of the confusion matrix.
* Wrote multiple functions to plot scatter plot graphs and saved 5 different csv files (one for each member) for each model.
* Pareto Optimal Frontier using varying classifiers: (change in parameters mentioned in brackets)
**  Rohan = DecisionTreeClassifier (min_samples_leaf=30). False Positive = 9, False Negative = 45
**  Manas = RandomForestClassifier (n_estimators = 100, max_depth = 5, min_samples_leaf = 5, criterion = entropy, random_state = 2). False Positive = 18, False Negative = 29. 
**  Aditya = AdaBoostClassifier. False Positive = 32, False Negative = 21. 
**  Adithya =  MLP Classifier. False Positive = 26, False Negative = 26.
**  Yisu = SVM (used svm.SVC, sigmoid = kernel). False Positive = 0, False Negative = 104. 

[[files/GrpahPresentation.png|center|thumb]] 

'''Individual Contribution:'''
* I decided to use the DecisionTreeClassifier as each one of us were responsible for implementing our own algorithms.
* Made changes to its parameter, min_samples_leaf=30, to ensure it has an optimal value and co-dominance with other algorithms.
* This then got me a good accuracy with an algorithm that fit well with the other four algorithms we planned on using.
* Learned more about different parameters of this model from the Sci Kit learn documentation. I had initially tried different models like more kernel functions and nearest neighbor algorithms, but none fit as well in with the other algorithms we planned to choose.
* I explored multiple functions and learned more about their parameter usage as well.
* Two other members and I thought of mapping alphabetical values to numerical ones to ensure optimized and accurate supervised learning algorithmic values for the final solution.

CSV File: https://drive.google.com/file/d/1ZWLsUhp5cQVxyHYLrqaqe6KKr5hG-_Yl/view?usp=sharing

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 18th, 2021
|-
|Review lecture slides
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 17th, 2021
|-
|Held team meeting #1
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 18th, 2021
|- 
|Held team meeting #2
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 19th, 2021
|- 
|Completed Python notebooks with 5 different co-dominant algorithms
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 19th, 2021
|}
== Week 3: September 8th - September 15th (2021) == 
===Self-Evaluation Form=== 
https://drive.google.com/file/d/1nv79npHVXmHw1cMCW4aEE8cNPSBdqY_t/view?usp=sharing

===Outline of class notes===

'''In class:'''
* Discussed 'Multiple Objectives – The MO in MOGA and MOGP'
* Overview of the Pareto Optimality 
* Rated our Python and ML Skills

'''Lecture Notes:'''
* Had an open discussion about what an algorithm looks for in a mate. There are a variety of objectives that we aim to achieve by generating specific algorithms to the get the best solution. Specificity, precision, accuracy, etc. are few important characteristics/goals that we aim to achieve.
* <b>Genetic Programming Cycle:</b> New Gene Pool → Evaluation → Genes with scores → Fitness Computation → Genes with fitness → Selection → Parental Genes → Mating → Child Genes → Mutation → New Gene Pool
* Keywords:
** Gene pool: The set of genome to be evaluated during the current generation
** Genome: Genotypic description of an individuals (DNA, GA = set of values, GP = tree structure, string)
** Search Space: Set of all possible genome. For Automated Algorithm Design it's the set of all possible algorithms
** The Evaluation of a Genome: Associates a genome/individual (set of parameters for GA or string for GP) with a set of scores.
*** From a location in search space: Genotypic description
*** To a location in objective space: Phenotype description
* <b>Important measures for multi objectivity: </b>
** True Positive – TP: How often are we identifying the desired object
** False Positive – FP: How often are we identifying something else as the desired object
** True Negative - TN: How often are we identifying the non-desired object
** False Negative - FN: How often are we identifying the desired object as something else
** Objectives: Set of measurements each genome (or individual) is scored against
** Objective Space: Set of objectives 
* Confusion Matrices use TP, FP, TN, FN to visualize outcomes.
* Other measures that we use for optimization and prediction:
** Sensitivity or True Positive Rate (TPR): AKA hit rate or recall, TPR = TP/P = TP/(TP+FN)
** Specificity (SPC) or True Negative Rate (TNR): TNR = TN/N = TN/(TN+FP)
** False Negative Rate (FNR):FNR = FN/P = FN/(TP+FN), FNR = 1 - TPR
** Fallout or False Positive Rate (FPR): FPR = FP/N = TN/(FP+TN), FPR = 1 – TNR = 1 - SPC
** Precision or Positive Predictive Value (PPV), PPV = TP / (TP + FP), Bigger is better
** False Discovery Rate, FDR = FP/(TP + FP), FDR = 1 - PPV, Smaller is better
** Negative Predictive Value (NPV), NPV = TN / (TN + FN), Bigger is better
** Accuracy (ACC), ACC = (TP+TN) / (P+N), ACC = (TP+TN) / ( TP + FP + FN + TN),  Bigger is better
** Objective Space:Each individual is evaluated using objective functions like Mean squared error, Cost, Complexity, True positive rate etc.
** Objective scores give each individual a point in objective space
** This may be referred to as the phenotype of the individual 
* All these measures help us in selecting the right algorithms, mating, fitness computation etc.
* <b>Pareto Optimality</b>
** An individual is Pareto optimal if there is no other individual in the population that outperforms the individual on all objectives
** The set of all Pareto individuals is known as <i>the Pareto frontier.</i>
** These individuals represent unique contributions. We want to drive selection by favoring Pareto individuals (But maintain diversity by giving all individuals some probability of mating).
* Two types of Multi objectivity algorithms:
** Non-dominated Sorting Genetic Algorithm II (NSGA II)
*** Population is separated into non-domination (nothing over performs it at all individuals and that is why it called Pareto optimal) ranks. Individuals are selected using a binary tournament and a lower Pareto ranks beat higher Pareto ranks.
*** Ties on the same front are broken by crowding distance (more diversity is better) (its better if points are pareto optimals but a little far from one another). Summation of normalized Euclidian distances to all points within the front. Higher crowding distance wins
** Strength Pareto Evolutionary Algorithm 2 (SPEA2)
*** Each individual is given a strength S and receives a rank R. S refers to the number of others in the population that it dominates and R refers to the sum of S’s of the individuals that dominate it. Pareto individuals are nondominated and receive an R of 0.
*** A distance to the kth nearest neighbor (𝛔k) is calculated and a fitness of R + 1/(𝛔k + 2) is obtained

===Lab 2 (Part 2): Multi-Objective Genetic Programming===
* Followed instructions given in the python notebook and tried to minimize the mean squared error along with the tree size.
* I also plotted the Pareto frontier. The main reason why we are trying to minimize area under the curve is to reduce distance of the Pareto curve with the axes and increase the probability of finding optimized and better individuals to minimize the two objectives.
* The area under the curve with the original parameters was: 2.463792426733847
* Best individual is: negative(cos(multiply(add(cos(sin(cos(multiply(add(cos(cos(x)), cos(add(multiply(x, x), sin(x)))), tan(x))))), cos(x)), tan(x)))) with fitness: (0.27530582924947056, 25.0)
[[files/2.2.1.png]] [[files/2.2.2.png]] [[files/2.2.3.png]]
* Now, our goal is to reduce AUC by at least 25% by changing the parameters.
* To achieve this goal, I attempted to remove the three trigonometric functions.
* First, when I just removed the tan function, the area under the curve increased; however, once I removed all three functions, the area under the curve reduced and became 0.6912744703006891.
* Best individual is: subtract(x, x) with fitness: (0.7223441838209306, 3.0). This was the best individual in this case.
[[files/2.2.2.3.png]] [[files/2.2.2.2.png]] [[files/2.2.2.1.png]]

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 8th, 2021
|September 15th, 2021
|September 11th, 2021
|-
|Review lecture slides
|Completed
|September 8th, 2021
|September 15th, 2021
|September 10th, 2021
|-
|Complete Self-Evaluation Form
|Completed
|September 8th, 2021
|September 15th, 2021
|September 10th, 2021
|- 
|Complete Lab 2 (Part 2)
|Completed
|September 8th, 2021
|September 15th, 2021
|September 12th, 2021
|}

== Week 2: September 1st - September 8th (2021) == 
===Outline of class notes===

'''In class:'''
* Overview of Genetic Programming.
* Learnt about Tree Representation.
* Briefly discussed Crossover and Mutation in Genetic Programming and Symbolic Regression.

'''Lecture Notes:'''
* Did a brief review of last week's material regarding Genetic Algorithms.
* Instead of taking an individual and having a function evaluator to obtain objective scores, the individual will be the function itself.
* Discussed one of the most common program structures in genetic programming, <b>Tree Representation</b>. Went over its uses and how to traverse over this tree. Parts of tree representation:
** Nodes are called primitives and represent functions, eg: +, -, x, /.
** Leaves are called terminals and represent parameters. The input can be thought of as a particular type of terminal. The output is produced at the root of the tree.
* To store the tree, the tree is converted to a lisp preordered parse tree and the order is determined by the inputs, so we use the root first and then go down.
* For example, if we use the following representation, we can determine how to traverse through the tree to determine the function by going top down and left to right.
[[files/TreeRepresentation1.png]]

So for this tree representation the function will be, f(x) = 3 * 4 + 1 and the parse would be [+,*,3,4,1].
* Crossover in tree-based GP is simply done by exchanging subtrees. Start by randomly picking a point in each tree and then these points and everything below create subtrees.
* The function with the best output mate and then produce the next generation of children.
* We also use the root sum (mean) square error method to calculate the optimized output and algorithms.
* Mutation can involve and is done by:
** Inserting a node or subtree
** Deleting a node or subtree
** Changing a node
* Learned more about the example of Symbolic Regression.
* Symbolic Regression is an example involving the mutation of GP that is used to evolve solutions using primitives.
* For instance: We use primitives like sin function, factorial, cos function, tan function, exponential, summation to evolve a solution to y = sin(x) by using the Taylor Series formula for sin(x).
* Finally we discussed that to evaluate the tree, we can measure the difference between the truth and the output that we obtain after feeding the a number of input points into the function to get outputs and then running f(x).

===LAB 2: Symbolic Regression===
* Tried to derive/inherit individuals from DEAP's PrimitiveTree instead of lists.
* Added primitives to the primitive set and chose a new mutation to find the most optimized solution.
* Compiled the primitive tree to generate the evaluation function, which finds the squared sum of the difference between our desired function's output and our individual's output. In this case we are trying to find minimum values that approaches zero faster.
* I tried modifying the code in three different ways:
** <u>The original program</u>:
[[files/Original Lab 2.png|center|thumb]]

*** First Attempt: Best individual is add(add(negative(negative(multiply(x, x))), multiply(subtract(multiply(x, x), negative(x)), multiply(x, x))), x), (1.0123748708878107e-16,)
*** Second Attempt: Best individual is add(add(add(multiply(add(multiply(multiply(x, x), x), multiply(x, x)), x), multiply(x, x)), x), subtract(x, x)), (8.59033944318508e-17,)
*** Depicts requirement for a more optimized and consistent solution to obtain minimum efficiently.
** <u>Program with added mutation</u>

*** First Mutation (mutInsert): Best individual is add(multiply(x, add(add(multiply(x, multiply(x, x)), x), multiply(x, negative(negative(x))))), x), (1.1585916677755867e-16,)

[[files/part2Lab2.png|center|thumb]] 

Even after running the code for a number of times the error is lesser and the solution is more consistent and optimized than the original program.

*** Second Mutation (mutShrink): Best individual is add(add(multiply(x, x), multiply(x, multiply(x, x))), add(x, multiply(x, multiply(multiply(x, x), x)))), (9.522005638492831e-17,)

[[files/part2lab21.png|center|thumb]]

Values were usually close to zero, but in some cases the value was very high (for maximum) and not as consistent. 
** <u>Program with only added primitives</u>

[[files/extralab2.png|center|thumb]]

*** Best individual is add(add(multiply(x, x), multiply(multiply(add(square(x), x), x), x)), x), (1.0172711918255375e-16,)

** <u>Program with both added primitives and mutation</u>
*** We used the mutInsert mutation as it seemed more consistent.

*** First pair of primitives (sin and square): Best individual is add(x, add(multiply(add(x, square(x)), multiply(x, x)), multiply(x, x))), (1.0123748708878107e-16,)
[[files/lab2primitive1.png|center|thumb]]

*** Second pair of primitives (absolute, cos): Best individual is add(add(cos(add(cos(subtract(x, x), cos(add(negative(x), subtract(x, negative(multiply(x, x)))), x)), x), x), cos(subtract(x, x), x)), add(x, x)), (0.0,)
[[files/finallab2.png|center|thumb]]

<b>In this lab, I attempted to reduce error to bring it almost to zero. This can be seen best in the graph where I used the absolute and cos primitives with the mutInsert mutation.</b>

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes (GP)
|Completed
|September 1st, 2021
|September 8th, 2021
|September 2nd, 2021
|-
|Review lecture slides
|Completed
|September 1st, 2021
|September 8th, 2021
|September 1st, 2021
|- 
|Complete Lab 2 (Part1)
|Completed
|September 1st, 2021
|September 8th, 2021
|September 4th, 2021
|}

== Week 1: August 25th - September 1st (2021) ==

===Outline of class notes===

'''In class:'''

* Overview of Automated Algorithm Design Wiki, syllabus, notebooks, etc.
* Started lecture on Genetic Algorithms.
* Briefly discussed the One Max Problem.

'''Lecture Notes:'''

* <u> The Concept </u>
** With genetic algorithms, each new generation is created through mating/mutation of individuals in the previous population  
** Their fitness is evaluated before mating/mutation.
** This fitness evaluation is done through the Fitness Proportionate method (randomization) or tournament method.
** Through numerous operations of the different processes, it will eventually produce the best individual 

* <u>Important Keywords</u>
** Algorithms: various evolutionary algorithms to create a solution or best individual.
** Individual: One specific candidate in the population (with properties such as DNA). In terms of programming this can be seen as a single solution to a problem.
** Population: Group of individuals whose properties will be altered. This refers to a number of solutions (set) for a problem.
** Objective: A value used to characterize individuals that you are trying to maximize or minimize. Usually the goal is to increase objective through the evolutionary algorithm.
** Fitness: Relative comparison to other individuals of the population.
** Evaluation: A function that computes the objective of an individual.
** Selection: This represents ‘survival of the fittest'. Preference given to better individuals, therefore allowing them to pass on their genes.
*** Fitness Proportionate: The greater the fitness value, the higher the probability of being selected for mating.
*** Tournament: Several tournament style competitions among various individuals. Winner selected for mating.
** Mate/Crossover: Represents mating between individuals.
** Mutate: Introduce random modification. The purpose is to maintain diversity.

* <u>The Process:</u>
** Randomly initialize population
** Determine fitness of population
** Select parents from population
** Perform crossover on parents creating population
** Perform mutation of population
** Determine fitness of population
** Repeat until best individual is good enough

===Lab 1: Genetic Algorithms with DEAP===

'''One Max Problem:''' This is a simple genetic algorithm problem and its objective is to convert all 0's and 1's in a vector/bitstring to all 1's. The outline and process completed is as follows (as per steps followed in the DEAP Python notebook):
* Imported the required modules (mostly DEAP tools)
* Defined individual classes, the fitness objective, and functions using DEAP's ToolBox.
* Represented bit string individuals as Booleans.
* Evaluated the population, ran the code repeatedly, performed tournament selection, mating, and mutation.
** Mating: Two-point crossover function
** Mutate: Flipping a bit in our bit string to either 1 or 0 respectively with an independent probability of flipping each individual bit of 5%.
* Ran the algorithm for 40 generations.
* <u>Conclusion:</u> Global maximum fitness of 100.0 was usually achieved in 40 generations but sometimes this was not the case; however, the maximum fitness obtained was always very close to 100.0. This method of tournament selection among 3 individuals is better than random search.

'''The N Queens Problem:''' The problem is to determine a configuration of n queens on a nxn chessboard such that no queen can be taken by one another. In this version, each queen is assigned to one column, and only one queen can be on each line. The outline and process completed is as follows (as per steps followed in the DEAP Python notebook):
* Imported the required modules (mostly DEAP tools)
* Defined individual classes, the fitness objective, and functions using DEAP's ToolBox.
* The fitness objective is to minimize the number of conflicts between 2 queens on the chessboard. Used to range function as well.
* Defined a function called "permutation" to help create our individuals and population and others like Evaluation (minimize diagonal conflicts), Selection (tournament selection of 3 individuals), Crossover (partially matched), and Mutation (shuffle indexes) functions.
* Ran main evolutionary loop for 100 generations. 
* Current algorithm did not always achieve the global minimum (0.0) but was consistently close.
* Visualization done to achieve more efficiency and speed using graphs as follows (100 generations graph):
[[files/FirstLab.png|center|thumb]]
* Iterations can be reduced with change is parameters and functions. This is clear as we can see that the minimum is easily obtained around the 25th generation. I am trying to work on making few changes in the algorithm for improved results by changing the mutation and mating probabilities.

For both problems we also found the basic statistics such as mean, minimum, maximum, and standard deviation, which helped to show that the final required output could be obtained in earlier generations itself with some tweaks made in the code.


'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install and set up DEAP library for Python
|Completed
|August 25th, 2021
|September 1st, 2021
|August 26th, 2021
|-
|Begin Notebook
|Completed
|August 25th, 2021
|September 1st, 2021
|August 25th, 2021
|- 
|Review Slides
|Completed
|August 25th, 2021
|September 1st, 2021
|August 26th, 2021
|-
|Complete Lab 1
|Completed
|August 25th, 2021
|September 1st, 2021
|August 29th, 2021
|}









